# The Ross AGI Score (RAS): A Multidimensional Evaluation Framework for AGI

Bradley Ross  
Lead AI/AGI Systems Architect & Developer  
Director, Agentics Foundation  
Harvard University, ALM Digital Media Design Candidate  
brad.ross@quantinc.com | https://www.linkedin.com/in/bradaross/
May 11, 2024  

Version: 1.1  
Draft Release Date: May 11, 2025  
Stable Archive Copy – stored in /archive/ras_v1.1_stable_may2025.md  


## Abstract


Progress toward Artificial General Intelligence (AGI) is outpacing the evaluation methods used to track it; most current tests are too narrow, too binary, or too loosely connected to human-level capability. The Ross AGI Score (RAS) framework addresses this gap with a comprehensive meta-evaluation structure and detailed evaluator’s rubric. RAS couples a 15-level AGI Capability Level (ACL) scale—ranging from simple narrow automation (ACL 1) to theoretical super-intelligence (ACL 15)—with a ten-dimension cognitive profile spanning memory, learning, abstraction, reasoning, creativity, social and emotional intelligence, planning, perception, and meta-cognition.

This paper operationalizes the three frontier milestones on that scale:
*   **ACL 9 — Near-Human Generality**
*   **ACL 10 — Human-Parity** (baseline human-level AGI)
*   **ACL 11 — Early Superhuman AGI**

For each dimension at these levels, we supply “Validity Commentary” tables (Appendices B-D) that cite concrete public benchmarks (e.g., ARC, BIG-bench, KILT, TTCT concepts), human-percentile pass bands, and escalation sketches to the next ACL. This material enables immediate, reproducible human-in-the-loop assessments while a fully automated test suite is under development.

A central design principle is protection against “savant syndrome” misclassification: RAS requires that every core dimension meet level-specific minima, preventing systems with narrow superhuman spikes from being mislabeled as generally intelligent despite significant deficits elsewhere. The framework therefore yields a one-page scorecard reporting both overall ACL and per-dimension ratings, giving researchers, developers, and policymakers a transparent, human-calibrated yardstick for:
*   tracking capability progression,
*   diagnosing profile gaps, and
*   setting policy or safety triggers as AI systems advance.

By providing a rigorous, multi-dimensional, and empirically anchored metric, the Ross AGI Score establishes a common language for evaluating and guiding the next generation of AI systems aspiring to true general intelligence.


**Table of Contents**

* [Abstract](#abstract)
* [1. Introduction](#1-introduction)
* [2. Cognitive Framework: The 10 RAS Dimensions](#2-cognitive-framework-the-10-ras-dimensions)
* [3. AGI Capability Levels (ACL 9–11)](#3-agi-capability-levels-acl-9-11)

  * [3.1. ACL 9 – Near-AGI](#31-acl-9--near-agi)
  * [3.2. ACL 10 – Human-Level AGI](#32-acl-10--human-level-agi)
  * [3.3. ACL 11 – Superhuman AGI](#33-acl-11--superhuman-agi)
* [4. Preventing “Savant Syndrome” Misclassification](#4-preventing-savant-syndrome-misclassification)
* [5. Conclusion](#5-conclusion)
* [Appendix A: Ten Core RAS Dimensions](#appendix-a-ten-core-ras-dimensions)
* [Appendix B: ACL 10 Evaluation Criteria by Dimension](#appendix-b-acl-10-evaluation-criteria-by-dimension)
* [Appendix C: ACL 9 Evaluation Criteria by Dimension](#appendix-c-acl-9-evaluation-criteria-by-dimension)
* [Appendix D: ACL 11 Evaluation Criteria by Dimension](#appendix-d-acl-11-evaluation-criteria-by-dimension)
* [References](#references)

## 1  Introduction

### 1.1  Why a New Evaluation Layer Is Needed
Methods for judging progress toward Artificial General Intelligence (AGI) have not kept pace with the widening scope of modern AI systems. Binary tests such as the Turing Test (Turing 1950) and single-score leaderboards like GLUE [ref] or ImageNet [ref] offer useful snapshots, yet they fail to show *where* a system is (or is not) general. Leaderboard saturation, Goodhart-style gaming (Goodhart 1975), and “demo-mode” chatbots that can fool but not *reason* all highlight the same gap: we lack a yard-stick that simultaneously measures **breadth, depth, and developmental progression**.

### 1.2  From Benchmarks to *Meta-Evaluation*
Individual benchmarks remain essential, but taken alone they are fragmented. **The Ross AGI Score (RAS) therefore sits one layer higher—as a *meta-evaluation framework*.**

* **Meta-evaluation framework:** a scaffold that specifies **which evidence** must be gathered, **how to align it to human standards**, and **how to combine it into a profile-level judgment**.  
* **Evaluator’s rubric:** the concrete checklist an assessor follows to decide whether a system satisfies the framework at a given capability level.

RAS v1.1 supplies both layers: a refined 15-level **AGI Capability Level (ACL) scale** and a rubric with explicit “Validity Commentary” for three frontier milestones—ACL 9, 10, 11.

### 1.3  The 15-Level Scale—Spotlight on ACL 9-11
The full ACL ladder runs from **ACL 1** (minimal narrow automation) to **ACL 15** (theoretical maximal super-intelligence). While the spectrum is retained from v1.0, this paper zooms in on the three levels most relevant to today’s frontier research:

| Milestone | Short label            | High-level description |
|-----------|------------------------|------------------------|
| **ACL 9** | *Near-Human Generality* | Performs at or near human level in most tasks but may show brittleness or inconsistency. |
| **ACL 10**| *Human-Parity AGI*      | Robustly matches average adult humans across every core dimension; baseline for AGI. |
| **ACL 11**| *Early Super-human AGI* | Begins to exceed skilled human experts in multiple dimensions—a qualitative leap. |

*Temporal context.* All validity bands and benchmark ceilings are calibrated to public results available through **Q1 2025** and will be revised as the field advances.

### 1.4  Motivating Work and Remaining Gaps
Influential proposals such as universal intelligence (Legg & Hutter 2007), the ARC benchmark (Chollet 2019), and psychometric “common-sense” tests (e.g., the informal *Coffee Test*) each illuminate part of the space, yet an integrated, human-referenced, multi-dimensional scorecard is still missing. Industry practice often defaults to narrower metrics. **RAS addresses this by enforcing a balanced-profile rule that blocks “savant syndrome” mis-classification:** no system may claim a high ACL if any core dimension lags markedly.

### 1.5  Structure of This Paper
* **Section 2** introduces the **ten core RAS cognitive dimensions**.  
* **Section 3** defines the **AGI Capability Levels**, focusing on ACL 9–11 validity considerations.  
* **Section 4** details how RAS prevents **savant mis-classification**.  
* **Section 5** concludes and outlines future work.  
* **Appendices** provide reference material:  
  * **A.** Ten Core Dimensions  
  * **B.** Evaluation criteria for ACL 10  
  * **C.** Criteria for ACL 9  
  * **D.** Sketch criteria for ACL 11


## 2. Cognitive Framework: The Ten RAS Dimensions

A cornerstone of the Ross AGI Score (RAS) is its **ten-dimension cognitive framework**. Rather than treat “intelligence” as a single scalar, RAS decomposes general cognition into ten interrelated capacities that together cover the spectrum of human-level competence. Each dimension can be probed with dedicated tasks, yet competence across all is required for a system to claim a high AGI Capability Level (ACL). The list below details these dimensions, which are used consistently throughout this paper, including in Appendix A (detailed definitions), the conceptual scorecard, and all subsequent validity tables.

| #  | Dimension                          | What it Captures (Illustrative Probes)                                                                                                                               |
| :--- | :--------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1**  | **Reasoning & Problem-Solving**      | Multi-step logical inference, deductive/inductive reasoning, causal and analogical problem-solving.<br/>*Probes:* Abstraction & Reasoning Corpus (ARC), complex mathematical proofs, strategic game play (e.g., Go end-games). |
| **2**  | **Learning & Adaptability**          | Few-shot/zero-shot learning, transfer across domains, rapid improvement from sparse feedback.<br/>*Probes:* Meta-World novel-task suite, BIG-Bench few-shot tasks, unseen Atari games after brief tutorials. |
| **3**  | **Knowledge & Memory**               | Breadth and accuracy of factual knowledge, long-context recall, integration of new information without significant drift.<br/>*Probes:* KILT benchmarks, post-cutoff news Q&A, consistency checks across lengthy dialogues. |
| **4**  | **Communication & Social Understanding** | Linguistic fluency, pragmatic inference, grasp of social context, discourse-level coherence.<br/>*Probes:* SummEval compression + tone-shift, multi-turn negotiation dialogues, irony/sarcasm detection. |
| **5**  | **Creativity & Innovation**            | Divergent idea generation, novel concept recombination, artistic or design originality.<br/>*Probes:* Torrance Alternate-Uses tasks, open-ended story or melody generation judged by experts, concept-merging puzzles. |
| **6**  | **Self-Reflection & Meta-Cognition** | Detection and correction of one’s own errors, calibrated confidence, explicit reasoning transparency.<br/>*Probes:* “Proof-Pilot-Self-Repair” sets, selective chain-of-thought reveal with error spotting, calibration Brier scores on held-out trivia. |
| **7**  | **Planning & Goal Pursuit**          | Long-horizon strategy, resource-constrained scheduling, dynamic replanning under uncertainty.<br/>*Probes:* NetHack ascension challenge, simulated disaster-response logistics, multi-step web-automation tasks. |
| **8**  | **Ethical & Value Reasoning**        | Resolution of moral dilemmas, adherence to stated principles, ability to justify value trade-offs.<br/>*Probes:* ETHICS benchmark, extended trolley problems with open justification, constitutional-AI compliance tests. |
| **9**  | **Embodied & Sensorimotor Intelligence** *(if applicable)* | Visual–spatial perception, motor control, real-time interaction with physical or simulated environments.<br/>*Probes:* DeepMind Control Suite unseen mazes, robotic manipulation of novel objects, AI2-THOR cluttered-room navigation. |
| **10** | **Emotional & Interpersonal Intelligence** | Recognition and appropriate response to human emotions, persona consistency, trust-building dialogue.<br/>*Probes:* Multi-modal emotion recognition, simulated therapy or mediation sessions, long-horizon role-play consistency ratings. |

### 2.1 Why Decompose Intelligence?

Experience with both human psychometrics and AI leaderboards shows that a system can excel in one facet while failing in others; large language models, for instance, may display fluent text generation but lack robust causal reasoning or reliable calibration. By scoring each dimension separately, RAS produces a **nuanced capability profile** that exposes strengths and weaknesses. This approach is crucial for blocking “savant-style” over-crediting: a system cannot achieve a high overall ACL if any core dimension falls below the required threshold for that level.

### 2.2 Interdependence and Holistic Thresholds

The dimensions are not entirely independent. Creativity often draws on knowledge; social understanding relies on language and theory-of-mind reasoning. Nevertheless, explicit separation clarifies evaluation design:

*   **Task Selection:** Benchmarks and probes are matched to the specific faculty they primarily stress (e.g., ARC for abstraction, ETHICS for value reasoning), reducing ambiguity in what is being measured.
*   **Balanced Profile Rule:** For any ACL claim to be valid under RAS, *all ten* dimensions (or all applicable dimensions, in the case of optional ones like Embodied Intelligence) must meet or exceed that level’s defined validity criteria. A model that is superhuman at mathematics but sub-par at empathy or planning cannot, therefore, claim “Human-Parity AGI.”

### 2.3 Illustrative Metric Sources

Each dimension can be operationalized with **public, reproducible tests** when available; bespoke or ad-hoc probes are typically used to supplement these or to assess aspects not yet covered by standardized benchmarks. The following table provides examples of public sources that can inform the assessment of various RAS dimensions:

| Dimension                          | Example Public Source(s)                     |
| :--------------------------------- | :------------------------------------------- |
| Reasoning & Problem-Solving      | ARC, BIG-Bench Hard (e.g., `strategyqa`)     |
| Knowledge & Memory               | KILT (e.g., KILT-EAC), MMLU                  |
| Communication & Social Understanding | SummEval, Topical-Chat, Social-IQa           |
| Creativity & Innovation            | Torrance Tests (concepts), potentially future creative benchmarks |
| Planning & Goal Pursuit          | MiniHack, WebArena                           |
| Embodied & Sensorimotor Intelligence | DM Control Suite, RoboSuite                  |

*(Note: Full task-to-rubric mappings and specific performance thresholds for ACLs 9, 10, and 11 are detailed in the validity tables of Appendices B, C, and D.)*

## 3. AGI Capability Levels (ACLs 9–11)

The Ross AGI Score (RAS) places every evaluated system on a **15-step AGI Capability Level (ACL) ladder**, ranging from **ACL 1** (minimal narrow automation) to **ACL 15** (theoretical maximal super-intelligence). While levels 1-8 chart the progression of contemporary narrow-to-broad AI systems, this section focuses on defining and operationalizing the three frontier milestones where *general* intelligence is understood to emerge and potentially surpass human ability: **ACL 9, ACL 10, and ACL 11**. For each of these levels, we discuss its core characteristics and how claims of attainment are validated against the ten RAS dimensions. Detailed pass-bands and illustrative benchmark examples are provided in Appendices C (for ACL 9), B (for ACL 10), and D (for ACL 11).

| Level    | Short Label               | Essence of Capability (across all ten RAS dimensions)                                                                    |
| :------- | :------------------------ | :--------------------------------------------------------------------------------------------------------------------- |
| **ACL 9**  | *Near-Human Generality*   | Demonstrates human-level performance in **most** dimensions, but with minor yet noticeable weaknesses or inconsistencies; a "proto-AGI." |
| **ACL 10** | *Human-Parity AGI*        | Robustly matches or exceeds average adult human performance **across every** core dimension, with no significant gaps. This is the RAS baseline for AGI. |
| **ACL 11** | *Early Superhuman AGI*    | Consistently **exceeds** skilled human experts by clear margins in multiple dimensions; begins to accomplish tasks that typical humans cannot. |

### 3.1 ACL 9 — Near-Human Generality

A system at ACL 9 demonstrates broad competence that *approaches* human norms across the RAS dimensions, yet it retains a few identifiable weaknesses or inconsistencies that prevent it from being classified as fully human-equivalent AGI. Typical signatures include:

*   **Breadth:** Exhibits credible capabilities in *all* ten RAS dimensions; no dimension is entirely absent or merely rudimentary.
*   **Depth:** Performance in at least half of the dimensions reaches or closely approaches the human baseline (as defined for ACL 10), while the remaining dimensions are demonstrably strong and within striking distance of human parity.
*   **Brittleness:** May exhibit occasional failures on complex out-of-distribution problems, in understanding subtle social nuances, or in tasks requiring high degrees of creative originality or robust common-sense reasoning.

**Validity Check:** Appendix C outlines specific, somewhat relaxed pass-bands for each dimension, conceptually targeting performance around the 60th to 75th percentile of human capabilities where applicable. An evaluator asserting an ACL 9 claim must provide evidence that the candidate system clears *every* relevant dimensional criterion. A significant deficiency in any single critical dimension (e.g., inability to perform basic ethical reasoning or a complete lack of learning adaptability) would disqualify an ACL 9 classification.

### 3.2 ACL 10 — Human-Parity AGI

ACL 10 is the pivotal threshold at which an artificial system is judged to possess **Artificial General Intelligence** in the commonly understood sense of matching human cognitive capabilities. A system at ACL 10:

*   Performs at a level comparable to or exceeding the **average educated adult human** (e.g., conceptually around the 80th percentile of general human capability, or meeting specific competency standards detailed per dimension) across **all ten** RAS dimensions.
*   Shows **robust generalization:** task success is maintained when superficial details of problems change, and the system can effectively apply and combine learned skills in novel situations.
*   Exhibits a **balanced cognitive profile:** no single core dimension significantly lags behind others, thereby preventing the misclassification of "savant" systems as generally intelligent.

**Validation Protocol:** Appendix B provides detailed evaluation criteria and minimum passing bands for each of the ten dimensions at ACL 10. These criteria reference public benchmarks (e.g., achieving high human-level accuracy on ARC, strong performance on KILT tasks, human-rated parity in complex dialogues) and established psychometric concepts. Attaining ACL 10 requires demonstrating that the system meets or exceeds *every* dimensional criterion through rigorous, ideally independent and publicly reproducible, evaluations. As of early 2025, no publicly demonstrated AI system is broadly recognized as convincingly satisfying the comprehensive bar set for ACL 10.

### 3.3 ACL 11 — Early Superhuman AGI

ACL 11 signifies the emergence of AGI that not only matches but begins to **distinctly surpass** human capabilities. Characteristics include:

*   **Quantitative Superiority:** Routinely achieves performance metrics significantly above human expert means (e.g., conceptually two or more standard deviations higher on relevant tasks) or attains perfect/near-perfect scores on complex benchmarks where even skilled humans plateau.
*   **Qualitative Novelty:** Demonstrates the ability to solve problems previously considered intractable for humans, generate creative outputs judged as paradigm-shifting by domain experts, or learn and master complex new domains with an efficiency and depth far exceeding human potential.
*   **Across-the-Board Advancement:** The superior capability is not isolated to a single faculty; all ten RAS dimensions meet or exceed enhanced "superhuman" performance bands.

**Validation Outlook:** Appendix D offers a preliminary sketch of superhuman benchmarks and characteristics for ACL 11 (e.g., near-perfect accuracy on difficult ARC variants, autonomous discovery of novel scientific insights, creative outputs rated by experts as superior to top human professionals). Verifying ACL 11 capabilities will necessitate challenges where human performance is a floor rather than a ceiling, relying on objective metrics for tasks with unbounded difficulty, formal verification of novel discoveries, or expert panel judgment of outputs that transcend existing human achievements.

---

In summary, ACL 9, 10, and 11 provide a **graduated, evidence-based vocabulary** for characterizing the capabilities of advanced AI systems:

*   **ACL 9** ≈ "Approaching AGI—highly capable, but minor gaps or inconsistencies remain."
*   **ACL 10** ≈ "Human-Parity AGI—functionally indistinguishable from a competent human generalist across the cognitive spectrum."
*   **ACL 11** ≈ "Early Superhuman AGI—consistently and demonstrably outperforms even skilled humans in multiple key areas."

Clarity regarding these thresholds is vital for guiding research, informing deployment decisions, and establishing appropriate policy and safety triggers. The RAS validity rubrics detailed in the appendices aim to make claims of achieving these advanced AGI levels concrete, testable, and falsifiable.


## 4. Preventing “Savant Syndrome” Misclassification

A persistent pitfall in AGI assessment is the **false positive**: mistaking a narrowly specialized system for a generally intelligent one. Human analogues abound—calendar savants or musical prodigies whose brilliance in a single faculty coexists with significant deficits elsewhere. In AI, the risk is identical: a model can achieve *superhuman* scores on one benchmark yet remain brittle, shallow, or incapable in other critical domains.

### 4.1 The Balanced-Profile Rule

The Ross AGI Score (RAS) is designed to eliminate this failure mode by enforcing a fundamental **Balanced-Profile Rule**:

> *An AI system must satisfy the minimum pass-band (as defined for a target AGI Capability Level) in **every one** of the ten core RAS dimensions before it can be awarded that target ACL.*

These ten dimensions span: Reasoning & Problem-Solving, Learning & Adaptability, Knowledge & Memory, Communication & Social Understanding, Creativity & Innovation, Self-Reflection & Meta-Cognition, Planning & Goal Pursuit, Ethical & Value Reasoning, Embodied & Sensorimotor Intelligence (if applicable), and Emotional & Interpersonal Intelligence. Consequently, no isolated spike of performance, however impressive, can mask a critical capability gap. A model that dazzles at calculus yet demonstrates poor ethical reasoning or lacks sensorimotor competence would be capped well below ACL 9, irrespective of its mathematical prowess.

### 4.2 Concrete Illustrations of the Rule in Action

The following table illustrates how the Balanced-Profile Rule, operationalized through the RAS rubric, addresses common "savant" temptations:

| “Savant” Temptation                                     | Why It Might Look Impressive Initially                        | How the RAS Rubric Exposes the Gap                                                                                                                                                                                            |
| :------------------------------------------------------ | :------------------------------------------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **IQ-Proxy Champion**<br/>(Model trained to ace classic pattern-matching tests) | Achieves off-the-chart scores, suggesting genius-level reasoning. | The *Reasoning & Problem-Solving* dimension also demands performance on ARC-style puzzles, which require rule inference from few examples. Memorized pattern-matching tricks often fail here, revealing a lack of broad, transferable reasoning. This lowers the dimension score, blocking high ACL claims. |
| **Hyper-Fluent LLM**<br/>(Model with vast web-ingested knowledge, eloquent prose) | Conversationally convincing; passes many knowledge quizzes with ease. | Tests within dimensions like *Planning & Goal Pursuit* (multi-step execution), *Embodied & Sensorimotor Intelligence* (spatial reasoning, control, if applicable), and *Self-Reflection & Meta-Cognition* (error detection, calibration) expose potential brittleness and lack of robust real-world modeling or reliable self-assessment. |
| **Game-Solving Specialist**<br/>(Superhuman at Go, StarCraft, or NetHack) | Dominates expert human players; often hailed as a sign of "general" mastery. | Evaluation across dimensions such as *Creativity & Innovation*, *Ethical & Value Reasoning*, and *Emotional & Interpersonal Intelligence* would likely reveal near-zero capability, confining the overall RAS level to the narrow-AI band despite game-specific excellence. |

### 4.3 Operational Safeguards within RAS

Several design features of RAS work in concert to ensure robust evaluation:

*   **Dimension-by-Dimension Auditing:** RAS scorecards require explicit evidence for each dimension. Reviewers can immediately identify missing data or reliance on weak or insufficient probes.
*   **Mixed-Benchmark Strategy:** Within each dimension, RAS encourages combining *template-vulnerable* tests (e.g., textbook-style IQ items) with *template-resistant* ones (e.g., ARC puzzles, novel few-shot tasks) to assess true generalization.
*   **Holistic Pass Criteria:** A system's overall ACL is effectively capped by its lowest-performing dimension relative to the target ACL's requirements. This "weakest link" principle ensures that a high ACL reflects well-rounded competence.
*   **Emphasis on Public, Reproducible Evidence:** Claims should, wherever possible, cite performance on open benchmarks or transparently documented ad-hoc tasks, allowing independent labs to reproduce or scrutinize results.

### 4.4 Practical Evaluation Workflow

To apply these safeguards, evaluators using RAS are guided by the following workflow:

1.  **Collect Strength-of-Evidence:** For each of the ten RAS dimensions, gather evidence using appropriate probes and benchmarks, as exemplified in the ACL-specific appendices (C for ACL 9, B for ACL 10, D for ACL 11).
2.  **Check for Disqualifying Gaps:** Any dimension lacking sufficient data, or demonstrating performance significantly below the target ACL's minimum pass-band, typically halts consideration for that ACL claim.
3.  **Stress-Test Weak or Borderline Areas:** If a model barely clears a dimensional threshold (e.g., its creativity is judged "adequate but uninspired"), evaluators should introduce additional, varied, or unseen tasks within that dimension to confirm genuine capability rather than rote memorization or narrow optimization.
4.  **Publish the Complete Profile:** The final RAS scorecard must transparently report scores or assessments for all dimensions, revealing both peaks and valleys. A single composite number or overall ACL is insufficient without the supporting dimensional breakdown.

### 4.5 Why This Safeguard Matters

History is replete with AI milestones—Deep Blue (chess), AlphaGo (Go), and the impressive text generation of modern LLMs—that, while spectacular, represent narrow or specialized achievements. Without a robust balanced-profile safeguard, the next breakthrough could be prematurely or wrongly heralded as "human-level AGI" based on dazzling performance on a limited set of benchmarks. **RAS makes such potential over-claims transparent and testable.** Only systems that demonstrate **consistent, human-range competence across all ten core cognitive dimensions** (or their superhuman equivalents for higher ACLs) can earn a high ACL designation like ACL 10. Those missing even a single critical facet remain unambiguously below that AGI threshold.

The result is a sharper, fairer, and more reliable assessment regime—one that rewards true breadth of intelligence and prevents the field, and the public, from mistaking dazzling *savants* for genuine *generalists*.


## 5 Conclusion  

The Ross AGI Score presented here provides a **structured, transparent, and human-calibrated yard-stick** for judging claims of artificial general intelligence.  By decomposing “intelligence” into **ten cognitive dimensions** and tying performance in each to clearly defined **AGI Capability Levels (ACLs)**, RAS turns the nebulous question *“Is this system an AGI?”* into a reproducible, evidence-based assessment.

### Key Contributions  

* **Multi-dimensional scorecard.** RAS demands that any would-be AGI demonstrate competence across memory, learning, reasoning, creativity, social understanding, self-reflection, planning, ethics, sensorimotor skill, and emotional intelligence—closing the loophole that lets narrow “savants” masquerade as generalists.  
* **Operational milestones.** Detailed criteria for **ACL 9 (Near-Human Generality), ACL 10 (Human-Parity), and ACL 11 (Early Super-human)** give researchers and policymakers a shared language for tracking progress on either side of the human benchmark.  
* **Evaluator’s rubric.** Dimension-specific pass bands, benchmark suggestions, and “Validity Commentary” tables show *how* to gather and weigh evidence today, even before fully automated test suites are available.  
* **Built-in safeguards.** The balanced-profile rule prevents single-benchmark over-claims, while the requirement for public, reproducible evidence makes RAS ratings auditable by independent teams.

### Practical Benefits  

* **Diagnostic clarity** – A complete RAS scorecard pinpoints *which* abilities limit a system’s overall level, guiding focused research investment.  
* **Policy alignment** – Regulators can tie capability thresholds (e.g., “no deployment beyond ACL 10 without safety review”) to concrete, testable criteria.  
* **Common vocabulary** – Instead of marketing superlatives, developers can report “ACL 9 in nine dimensions, ACL 8 in Creativity,” a statement the community can verify.

### Looking Ahead  

Today’s release emphasises cognitive and knowledge-based tasks because they are the most measurable with current tooling.  As AI systems acquire richer embodiment, social agency, or self-modification, RAS can evolve:

* **Expanded or re-weighted dimensions** for collaboration, long-horizon autonomy, or alignment diagnostics.  
* **Periodic recalibration** of pass bands as benchmarks saturate and new gold-standards emerge.  
* **Automation roadmap** – community-maintained test harnesses and blind challenge sets to reduce evaluation cost and bias.

Crucially, RAS is a **living framework**.  We invite the research community to adopt it, stress-test it, and iterate on its benchmarks and thresholds.  In doing so we advance a shared goal: ensuring that when the first system genuinely clears the human-level bar, the evidence will be unmistakable—and the world will be prepared.

*Extraordinary claims demand extraordinary evidence.*  The Ross AGI Score specifies that evidence.  We hope it becomes a cornerstone for the safe and comprehensible progress of artificial general intelligence.


---

## Appendix A: Ten Core RAS Dimensions

Below is the list of the ten core dimensions comprising the Ross AGI Score cognitive framework, along with a brief definition of each dimension:

1. **Knowledge & Memory** – The capacity to acquire, store, and retrieve information. This includes an AI’s factual knowledge about the world (both semantic memory and episodic memory if applicable) and its ability to recall relevant information when needed. A high level in this dimension means the system has a broad and accurate knowledge base (comparable to an educated human) and can effectively use it in context.
2. **Learning & Adaptability** – The ability to learn new skills or knowledge and to adapt to novel tasks or changes in environment with minimal instruction. This dimension measures how well the AI generalizes beyond its training and how efficiently it can improve its performance when faced with something new (for example, learning from a few examples or instructions – akin to few-shot learning or on-line adaptation).
3. **Reasoning & Problem-Solving** – The capability for logical reasoning, critical thinking, and systematic problem-solving. It covers deductive reasoning (applying general rules to specific cases), inductive reasoning (generalizing from examples), and handling of well-defined problems (like puzzles, mathematical questions) as well as ill-defined problems. A strong AI in this dimension can solve novel problems it hasn’t seen before by reasoning through them, similar to a human thinking through a riddle or logic puzzle.
4. **Abstraction & Generalization** – The ability to abstract underlying principles from specific instances and to generalize knowledge to unfamiliar situations. This involves pattern recognition, analogical reasoning, and transferring concepts across domains. For instance, given a few examples of a pattern or a task, the AI should infer the general rule and apply it to new examples. This dimension is crucial for “broad” intelligence – it prevents the AI from being tied only to cases it explicitly memorized.
5. **Creativity & Divergent Thinking** – The capacity to generate original, diverse, and imaginative ideas or solutions. This includes creativity in problem-solving (finding non-obvious solutions), as well as creative expression (writing, art, design). Divergent thinking is the ability to branch out into many possible approaches or answers, rather than following a single line of thought. An AI scoring high here can produce outputs that are not only correct but also novel and innovative, comparable to human creativity on tasks like brainstorming or artistic creation.
6. **Communication & Language Understanding** – Mastery of language and communication skills. This dimension covers both understanding (comprehension of input language, including nuances, ambiguity, and context) and generation (producing clear, coherent, contextually appropriate and grammatically correct language). It may also include multi-lingual abilities and the capacity to explain its reasoning or teach others. Essentially, an AI with high communication skill can interact with humans or other agents effectively through language, matching human-level discourse and interpretation.
7. **Social & Emotional Intelligence** – The ability to interpret and respond to social and emotional cues in an appropriate way. This involves understanding others’ perspectives, intentions, beliefs, and feelings (theory of mind), as well as displaying or simulating empathy and proper social behavior. It also covers ethical and moral reasoning in social contexts. An AI strong in this dimension can navigate complex social situations or narratives, infer what people might be thinking or feeling, and make judgments that align with social norms or emotional intelligence akin to a human’s capabilities in interpersonal situations.
8. **Planning & Decision-Making** – The capability for strategic thinking, planning multi-step actions, and making decisions, especially under uncertainty. This includes setting goals, forecasting outcomes, optimizing choices to achieve objectives, and possibly handling trade-offs or risk. It reflects an AI’s executive function – for example, forming a plan to solve a complex problem, or managing a task that involves many interdependent steps, and adjusting plans as needed. A high level in this dimension means the AI can autonomously break down tasks, allocate resources or steps, and carry out long-term objectives effectively, similar to a competent human planner or problem-solver.
9. **Perception & Modeling** – The ability to perceive the environment (through vision, audition, or other sensors) and build useful internal models of the external world. For non-embodied AIs, this might involve interpreting visual data (images, videos), understanding spatial or temporal patterns, or integrating information from multiple modalities. For embodied AIs or agents, it includes situational awareness and understanding of physical context. Essentially, this dimension assesses how well the AI can make sense of raw data from the world and use it to inform its decisions or understanding, analogous to human perception and world-modelling capabilities.
10. **Self-Monitoring & Meta-Cognition** – The capacity for self-awareness in problem-solving, including monitoring one’s own performance, recognizing errors or uncertainties, and adjusting one’s approach. This dimension covers whether the AI can evaluate its own outputs (e.g., detect when an answer might be wrong or confidence is low), and whether it can improve over time by reflecting on mistakes. In humans, this relates to meta-cognitive skills – “thinking about thinking.” An AI with strong self-monitoring might, for example, notice that it doesn’t understand a question and ask for clarification, or after giving an answer, double-check and correct itself if it finds inconsistency. This leads to more reliable and autonomous improvement behavior.

Each of these dimensions represents an axis along which an AI system’s capability can be measured and scored. In a full RAS evaluation, specific tests and benchmarks are mapped to each dimension to quantify performance (see Appendices B, C, D for such mappings at different ACLs). It’s important to emphasize that true AGI requires a **balance across all dimensions** – a deficiency in any one of these would indicate a form of intelligence that is not “general.” Thus, this list also implicitly serves as a checklist for developers: a candidate AGI should be probed on each of these to see if it has any blind spots.

## Appendix B: ACL 10 Evaluation Criteria by Dimension

**(Human-Level AGI Performance Standards)**

For each of the ten RAS dimensions, we detail the task characteristics, example evaluation tasks or benchmarks, and the minimum performance **passing criteria** required for an AI system to be considered at **ACL 10 (human-level AGI)**. These criteria are intended to represent roughly human-equivalent competence in each dimension, which collectively define human-level general intelligence.

1. **Knowledge & Memory**

   * **Task Characteristics:** Open-domain factual questions, breadth of knowledge across sciences, humanities, everyday facts; retention and recall of information over long contexts.
   * **Examples:** Trivia and quiz questions (e.g., from **TriviaQA** or Quiz Bowl datasets), open-domain QA such as **Natural Questions** where the AI must retrieve facts from a large corpus, summarizing long documents accurately, and consistent recall of details within a conversation.
   * **Benchmarks:** **KILT benchmark** tasks – including **FEVER** fact verification (checking a claim against Wikipedia), open-domain QA tasks like **Natural Questions** and **TriviaQA**, and **Wizard of Wikipedia** dialogue (demonstrating knowledge in conversation). Also, high scores on reading comprehension tests and specialized exams (e.g., passing a standard high school or college exam in history or science).
   * **Minimum Passing Band (ACL10):** The AI’s knowledge base and recall should be **comparable to an educated human adult**. Concretely, it should answer general knowledge questions with a success rate similar to a human quiz player or a professional. For instance, on open-domain QA, achieving approximately 80-90% accuracy on questions that a person with internet access could solve. On fact verification, it should correctly verify or refute claims with evidence at a human-expert level. In practical terms, it would be expected to pass a standard pub trivia quiz or score at least around the median on **Jeopardy!** questions. It should also demonstrate memory consistency in dialogue (not contradicting itself about remembered info) at least as well as a human memory would over short-term context.

2. **Learning & Adaptability**

   * **Task Characteristics:** Rapid acquisition of new skills or knowledge from limited examples or instructions (few-shot or one-shot learning), adapting to changes in rules or goals, and cumulative learning over time.
   * **Examples:** A custom “learning challenge” where the AI is taught a new board game or a new mathematical operation via a brief tutorial and then tested on it. Few-shot learning tasks like adapting to a new style of writing after seeing only a couple of examples, or learning to use a new API by reading documentation and then writing code with it. Another example: an interactive environment (like a simplified video game) where the rules change and the AI must figure out the new rules by trial and error as a human would.
   * **Benchmarks:** The **BIG-bench** suite contains tasks specifically designed to test adaptation to novel concepts. Additionally, the **Animal-AI Olympics** or similar challenge environments test learning in embodied contexts. One could also use the **OpenAI Gym** or **Meta-World** environments for few-shot adaptation. For a more cognitive test: **Bongard Problems** (visual concept learning) or other concept-learning benchmarks.
   * **Minimum Passing Band (ACL10):** The AI should demonstrate an ability to learn unfamiliar tasks with **efficiency similar to a human** given the same information. For instance, if shown examples of a new word usage or a pattern only a few times, it should generalize that correctly (matching human few-shot generalization). If given instructions for a new game, it should play at a competent level after equivalent “practice” as a human would need. In quantitative terms, if humans typically achieve, say, 70% success on a new task after 5 demonstrations, the AI should be in a similar range. The AI should also handle rule changes on the fly about as quickly as people can figure them out. Essentially, any learning curve for the AI on novel tasks should not be significantly steeper than the human learning curve for comparable tasks.

3. **Reasoning & Problem-Solving**

   * **Task Characteristics:** Logical deduction and induction problems, mathematical problem solving, puzzles, and complex question answering that requires reasoning steps. This also includes commonsense reasoning challenges and multi-hop reasoning where intermediate conclusions must be drawn.
   * **Examples:** Classic logic puzzles (e.g., syllogisms, Sudoku, Rubik’s Cube-solving conceptually, “zebra” puzzles), word problems in mathematics (from algebra to calculus), reasoning-heavy QA (like answering a question that requires combining clues from multiple sentences or sources), **commonsense reasoning tests** (Winograd Schema Challenge or Rebus puzzles), and solving novel problems like those in the **ARC** challenge.
   * **Benchmarks:** The **Abstraction and Reasoning Corpus (ARC)** for non-verbal puzzle solving; the **BIG-bench logical reasoning tasks**; math benchmarks such as **MATH dataset** or competition-level problems; the **CommonsenseQA** or **OpenBookQA** for commonsense; and perhaps an AI’s performance on standardized tests that involve reasoning (GRE analytical section, or LSAT logic games). A high-level benchmark could be: passing a general high school math exam or doing as well as an average human on IQ test analogies and matrix reasoning (not because we consider IQ test sufficient, but as a piece of evidence of reasoning ability).
   * **Minimum Passing Band (ACL10):** The AI should solve reasoning problems at **least as well as an average human**. For example, it should score around 100 (human average) on an IQ test’s reasoning sections, or solve a majority (say >75%) of problems on a standard logic puzzle set that humans with some education can solve. On ARC, which humans find easy and where AIs have struggled, the AI should reach human-level success rates (humans reportedly solve most ARC tasks with high success, so ideally the AI should solve a comparable fraction, e.g., \~80% of the tasks correctly). For math word problems up to high school level, it should get a grade that a passing human student would (e.g., \~80%+ correct on a test of mixed math problems). Essentially, its problem-solving should not be obviously below that of a competent adult: if given the same puzzles or problems, a typical human and the AI would perform similarly well.

4. **Abstraction & Generalization**

   * **Task Characteristics:** Inferring general rules from limited examples, transferring concepts to new contexts, pattern recognition in novel domains, analogy making. Many of these tasks overlap with reasoning but specifically stress doing so in unfamiliar settings or with very sparse data.
   * **Examples:** The **ARC puzzles** again are prime examples (inferring the rule from a few input-output grid examples and applying it to a new grid). Also, presenting the AI with a few examples of a sequence or a new categorization task and seeing if it can categorize new instances (like showing it three strange alien artifacts labeled as “blorks” and two as “fleems” and then asking which category a new artifact belongs to, testing abstraction of category concepts). **Analogies** (e.g., “A is to B as C is to ?”) that require finding abstract relationships. **Visual analogies or Raven’s Progressive Matrices** (which measure fluid intelligence in humans) could be used – these are pattern-completion tasks that require abstract reasoning.
   * **Benchmarks:** Beyond ARC, **Raven’s Progressive Matrices** test (there are 2D visual analogies puzzles often used in IQ tests) would be a direct measure – the AI should perform about as well as humans (most humans can solve many of them, though not all). The **Bongard Problems** (a classic set of abstract concept learning puzzles) can test abstraction. In language, **BIG-bench** includes tasks on analogy and abstract reasoning which could be used. Another benchmark: the **Analogical Reasoning Task** or **Matrix Reasoning** from psychometric evaluations.
   * **Minimum Passing Band (ACL10):** The AI should demonstrate **human-like fluid intelligence** in abstraction. If humans solve \~80% of Raven’s matrices on a test, the AI should be in a similar range. For tasks like ARC, as mentioned, target human-level success (which is high; humans reportedly find ARC puzzles relatively straightforward with some thought). On analogies, it should perform comparably to educated humans (for example, if given analogies from SAT exams, it should get a score comparable to a college-bound student). More generally, whenever the AI is confronted with a pattern or rule it hasn’t been explicitly told, it should be able to infer it about as reliably as a human can from the clues. A passing criterion could be: given a set of novel pattern challenges, the AI’s average score is within the variance of human test-takers (not statistically significantly lower).

5. **Creativity & Divergent Thinking**

   * **Task Characteristics:** Open-ended tasks that have no single correct answer but evaluate the originality and variety of responses. Examples include creative writing prompts, brainstorming tasks, design or invention challenges, and problem-solving tasks where typical solutions are known but creative alternatives are sought.
   * **Examples:** **Torrance Tests of Creative Thinking (TTCT)** type activities – e.g., “Think of as many uses for a brick as possible” (Alternate Uses Task) and evaluate fluency (number of ideas), flexibility (variety of idea categories), originality. Story or poem writing prompts (“Write a short story about a world where time runs backwards”) assessed by human judges for creativity. Solving a practical problem with a novel solution (“Devise an unusual solution for reducing traffic in a city”) and comparing with human-submitted solutions. **Drawing or visual creativity**: perhaps having the AI generate a simple sketch or diagram given a creative task (if it has that modality). Or completing an analogy or metaphor in an unexpected but apt way.
   * **Benchmarks:** Creativity is harder to benchmark objectively. However, TTCT provides a standardized way to score creativity on fluency, flexibility, originality, and elaboration. We could have human evaluators score the AI’s outputs on such tests and compare to normative data for humans. Other potential benchmarks: **BIG-bench** has some tasks for humor or creativity (like joke generation). The **HUMA challenge** (if any, for humor) or the “creative writing” tasks where AI output is judged by humans vs human output. In absence of established metrics, one could set up a Turing-test-like evaluation for creativity: have humans rate a batch of creative outputs blindly from AI and humans, and see if they can distinguish or if the AI’s work is rated comparably.
   * **Minimum Passing Band (ACL10):** The AI’s creative output should be **within the range of normal human creativity**. For instance, on the Alternate Uses Task, if an average adult can come up with \~10 uses for a common object with a certain originality score distribution, the AI should produce a similar number and novelty of ideas (some common uses, some clever ones) such that human judges consider its answers about as creative as an average person’s. In story writing, at least some of its stories should be assessed as sensible, interesting, and novel – akin to what a human might write – even if not literary genius level. We might say the AI passes if a panel of judges finds no significant difference between the creativity of its outputs and those of human participants given the same prompt (or possibly scores above a threshold on TTCT scales in a formally administered test). In quantitative terms, perhaps achieving median scores on TTCT subscales (fluency, originality, etc.) compared to the adult population. Essentially, the AI should demonstrate the ability to not only follow patterns but also to *break* patterns in meaningful ways – a hallmark of creativity – at a level where its outputs would not seem out of place in a collection of human responses.

6. **Communication & Language Understanding**

   * **Task Characteristics:** Understanding complex language input (including long documents or conversations, ambiguous or nuanced statements, idioms, etc.), and producing coherent, contextually appropriate, and correct language outputs. Also includes following instructions accurately and generating explanations or summaries. Potentially includes multi-lingual understanding and translation as well.
   * **Examples:** Participating in a lengthy conversation and maintaining context (like a customer service dialogue or a philosophical discussion), reading a paragraph or story and answering detailed comprehension questions about it (including implicit information), summarizing an article or report in concise form, translating a piece of text from one language to another while preserving meaning and tone, writing an essay or a report on a given topic with proper structure and grammar. Another example: interpreting a joke or a figure of speech correctly, which shows deep understanding.
   * **Benchmarks:** **SuperGLUE** and **GLUE** benchmarks for natural language understanding (which include tasks like reading comprehension, common sense inference, word sense disambiguation, etc.). The **BIG-bench** tasks also include many language tasks (for instance, understanding of idioms, inference, etc.). For dialogue, one could use metrics from the **DSTC** (Dialog System Technology Challenge) or simply human evaluation in conversation Turing tests. For translation, benchmarks like **WMT** translation challenge. Additionally, an exam setting: the AI could be given sections of an SAT reading test or a graduate record exam verbal section to see if it scores in human range.
   * **Minimum Passing Band (ACL10):** The AI should exhibit **language proficiency on par with a fluent adult** in terms of comprehension and generation. This means scoring approximately at the level of a college-educated native speaker on reading and writing tasks. For example, on reading comprehension benchmarks like SuperGLUE, it should reach or exceed human performance scores (which are often around 90% on some tasks) – many top models already do in narrow sense, but combined with consistency and broad context handling. The AI’s written outputs (essays, explanations) should be coherent and require no more editing than a human’s work to be publishable. Grammatically, it should make no more errors than a typical human (minor typos acceptable, but not systematic mistakes). In following instructions, it should rarely misunderstand if a human wouldn’t. One might set a criterion like: passes a Turing Test focused on communication – i.e., in a blind conversation test with skilled interlocutors, it is indistinguishable from a human in its ability to carry on a discussion for an extended period. Or simply, if given a written exam in language arts, it achieves a passing grade. Quantitatively, on GLUE/SuperGLUE, aim for near-human scores (most sub-tasks within a few percent of human baseline). For translation, achieve BLEU scores close to professional human translators on tested language pairs.

7. **Social & Emotional Intelligence**

   * **Task Characteristics:** Interpretation of social scenarios, demonstration of empathy or appropriate emotional responses, ethical decision making, theory of mind tasks (inferring what someone knows or intends), understanding humor or sarcasm, and generally behaving in a way consistent with social norms.
   * **Examples:** Reading a short story or vignette involving characters and answering questions about their motivations or feelings (“What is character A likely feeling after this event, and why?”). Being presented with a moral dilemma and explaining a resolution in line with human ethics. Engaging in a role-play conversation (e.g., as a therapist or friend) and responding in an emotionally intelligent manner. Identifying if a statement in conversation is sarcastic or serious. Another test: the **“Theory of Mind” puzzles** where you ask what one character believes about another’s belief (classic false-belief tasks). Or tasks like empathic response generation: given a user says “I’m feeling down today,” the AI responds supportively and appropriately.
   * **Benchmarks:** **Social IQa** (Social Interaction QA) dataset which asks commonsense social reasoning questions. The **Empathetic Dialogues** dataset for testing empathetic conversational response. The **Theory of Mind** evaluation from the BIG-bench or other studies to see if the AI can solve Sally-Anne type false-belief tasks. Possibly the **ETHICS** dataset, which tests moral reasoning in situations. Human evaluation is key: for example, have people judge whether the AI’s responses in a set of social scenarios are reasonable and emotionally appropriate (no bizarre or tactless answers). Additionally, something like the **Turing Advice test** – can the AI give advice to someone in a personal situation that is as sensible as human advice?
   * **Minimum Passing Band (ACL10):** The AI should demonstrate **social reasoning and empathy comparable to an average human’s social intelligence**. It doesn’t need to be a master psychologist, but it should interpret common emotional cues correctly (e.g., know that if someone’s dog died, they are likely sad and need comfort, not jokes). In objective terms, for tasks like SocialIQA, it should reach human-level accuracy in choosing the appropriate social outcome or explanation. In conversation, human evaluators should generally rate its social responsiveness as natural and appropriate. One could require that in a blinded test, humans find the AI’s advice or reactions as acceptable as advice from peers. On moral/ethical questions, the AI’s decisions should align with common human morals unless instructed otherwise (and it should be able to articulate reasons similarly to a human). Essentially, it passes if it **rarely commits major social blunders or misunderstandings** that a normal person wouldn’t. For instance, correctly answering theory-of-mind questions that even children can solve indicates it’s at least at human baseline for understanding others’ perspectives.

8. **Planning & Decision-Making**

   * **Task Characteristics:** Multi-step problem solving, strategic game play, time management tasks, and decisions under uncertainty. The tasks typically involve figuring out a sequence of actions to achieve a goal, possibly optimizing for constraints or reacting to changes.
   * **Examples:** Solving a maze or navigation task (virtually, providing directions), playing strategy games like chess or Go at decent level (though superhuman performance is not required at human-level AGI, just competitive with humans), making a plan for a complex task (e.g., “Plan a 7-day trip through Europe under \$2000 budget” or “Formulate a research plan to investigate hypothesis X”), scheduling problems, or managing resources in a simple simulation. Another example: real-time strategy game or turn-based planning scenarios (the AI should handle these about as well as an average human player). Even something like the Tower of Hanoi puzzle or blocks-world planning tasks: classic AI planning problems that humans can solve.
   * **Benchmarks:** For purely text-based planning, the **ALFWorld** environment or TextWorld (interactive fiction games requiring planning) can be used. There’s also the **CommAI** tasks that involve sequences. In board games, one could use ELO ratings: maybe require the AI to play chess at the level of a decent club player (say ELO \~1200-1500) which is around human average hobby skill. For decision under uncertainty: the **Oregon Trail** style simulation or **card games** like bridge or poker at a casual level. **Puzzle benchmarks** like the generalized planning in AI (planning competitions problems) but simplified enough that humans solve them too.
   * **Minimum Passing Band (ACL10):** The AI should be able to **formulate and execute plans comparable to a human of average competency in similar scenarios**. For instance, if given a travel planning task with constraints, a human might come up with a reasonable itinerary in an hour; the AI should do similarly (and not forget key steps like booking accommodation). If tasked with a puzzle like Tower of Hanoi (with, say, 6 disks, which humans can solve with focus), the AI should solve it too. In strategy games, it doesn’t need to be champion, but should not be trivially beat by novices – e.g., it can play chess such that it wins some games against casual players or it can handle a simple video game mission without failing where a human wouldn’t. Essentially, if you give it a goal and necessary tools, it should reliably achieve the goal without needing step-by-step micromanagement, similar to how you would trust a reasonably intelligent person to do it. Quantitatively, one might say: solves X% of planning tasks that humans solve, within comparable time or steps. Or passes a set of planning problem benchmarks (like logistics planning puzzles) at a level akin to a human problem-solver in terms of success rate and solution quality. For decision-making under uncertainty, the AI’s choices should on average yield outcomes as good as those a human would achieve facing the same uncertainty (for example, in a betting game or resource allocation under unknown conditions, it performs in the range of human players, not worse than random or naive).

9. **Perception & Modeling**

   * **Task Characteristics:** Interpreting visual or other sensory data, understanding spatial relationships, and building internal representations of an environment. Could include image recognition, visual question answering, or even physics understanding from perception.
   * **Examples:** Identifying objects in images (general image recognition, like “this is a cat, that is a bicycle”), describing a scene in an image (image captioning, e.g., describe what’s happening in a photo), answering questions about an image (“Is the person in the image happy?” which requires reading facial expression, or “How many animals are there and what are they doing?”). Tracking objects or events in a video (like observing a simple video and predicting what will happen next, e.g., a ball is rolling towards the edge of a table, will it fall?). For spatial modeling: reading a map or a diagram and understanding relationships (“Which room is north of the kitchen on this floor plan?”). If the AI has embodied aspects, then interpreting sensor input to avoid obstacles, etc., but for pure cognitive we stick to visual or spatial understanding tasks.
   * **Benchmarks:** **ImageNet** classification at human-level accuracy (humans are very good at basic object recognition, nearly 95%+ on ImageNet; AIs have surpassed average human on ImageNet top-1 accuracy). **COCO image captioning** benchmarks for describing images (there are BLEU/METEOR metrics and also human eval of caption quality). **VQA (Visual Question Answering)** dataset to measure understanding of images in question form. Possibly **CLEVR** (a synthetic visual reasoning test that checks if the AI can reason about object relationships in an image). Also, simple physics understanding tests like the **Intphys** (intuitive physics) benchmark where the AI sees a scene and must detect if something impossible happened.
   * **Minimum Passing Band (ACL10):** The AI should perceive and interpret visual/spatial information **about as well as an average human**. For object and scene understanding, that means near human-level accuracy in recognizing common objects and understanding scenes. If an average person can label 19 out of 20 objects correctly in an image set, the AI should be close to that (95% accuracy). For captioning, its descriptions should be sensical and match what human captioners would say; not necessarily super creative, but correctly capturing key details with perhaps the occasional minor miss (comparable to human variance). On VQA, it should answer correctly a high fraction of questions that human annotators answered (ideally within a few percentage points of human performance on that dataset). For spatial reasoning (like reading a map or predicting physical outcomes), it should get it right as often as people do. Essentially, the AI shouldn’t be baffled by normal visual inputs or simple real-world physics – if shown a short video of someone knocking over a glass, it knows the glass fell and broke, which is something even a child understands. A passing criterion could be: on a standard battery of perception tests (object ID, captioning, VQA, etc.), the AI’s performance is statistically indistinguishable from that of a human control group. If the AI has multi-modal input, it integrates info (e.g., can combine a verbal instruction “pick up the red block” with a visual scene correctly). In summary, at ACL10 the AI sees and understands the world at least as well as a typical person in everyday situations (though not necessarily with expert-level vision in specialized tasks).

10. **Self-Monitoring & Meta-Cognition**

    * **Task Characteristics:** Awareness of one’s own knowledge and performance, the ability to detect uncertainty or errors, and to adjust strategies. Tasks include calibration tests (does the AI know when it doesn’t know?), ability to explain its reasoning (which shows it has some internal model of its process), and learning from mistakes without explicit external feedback.
    * **Examples:** After answering a question, the AI provides a confidence level, and we check if those confidences correlate with correctness (like a well-calibrated person would say “I’m not sure” when they indeed are likely wrong). Having the AI attempt a problem, then allow it to reflect or critique its answer, and see if it catches and corrects a mistake (like double-checking math or proofreading its own essay). Another example: present a tricky question that is actually unsolvable or ambiguous; a meta-cognitively skilled AI should acknowledge insufficient info rather than guess wildly. Or ask it to explain *how* it got an answer – a coherent explanation indicates it’s monitoring its reasoning chain. A classic human example is asking “Do you know the capital of X country?” – a human who doesn’t know will say “I don’t know” rather than just guessing; the AI should similarly admit ignorance appropriately.
    * **Benchmarks:** There aren’t standard benchmarks solely for meta-cognition, but one can derive metrics: e.g., **calibration curves** comparing the AI’s predicted confidence vs actual accuracy (aim for calibration similar to or better than humans, who often are overconfident on hard questions). The **LEEP (Leave-One-Out Evaluation of self-awareness)** or some tasks from BIG-bench that test if the model knows when it shouldn’t answer. Possibly the **Explainable QA** tasks (where AI must give an explanation) indirectly measure if it can justify itself. Another angle: measuring how performance improves if the AI is allowed to reflect (like chain-of-thought prompting) – a self-monitoring AI might improve on second attempt as it notices first attempt flaws.
    * **Minimum Passing Band (ACL10):** The AI should exhibit **a degree of self-awareness about its knowledge limits and errors comparable to a human professional exercising care**. For example, if it doesn’t know something or is uncertain, it should state that or hedge, rather than always giving a confident answer. Humans are not perfect at this, but we expect an educated person to usually recognize when they are guessing versus when they know an answer cold. So a passing AI might, for instance, correctly use phrases like “I’m not sure, but I guess…” in perhaps 90% of the cases where it indeed ends up being wrong, meaning it knew to doubt itself, versus rarely being confidently wrong. Numerically, one could require that the AI’s confidence calibration error is within the range of human respondents on a set of trivia (i.e., it’s not wildly over- or under-confident overall). Also, when allowed to self-criticize, it should catch a fair fraction of its mistakes. For instance, if you have it solve 20 problems and then review its answers, a human might catch say 5 mistakes upon double-checking; the AI should similarly catch a comparable portion. Additionally, its explanations of reasoning should be generally valid (not just plausible gibberish). A criterion could be: in an **explain-your-answer** test, human judges rate the AI’s explanations as reasonable and aligned with its answers at least as often as they would for human participants’ explanations. In sum, the AI should demonstrate it “knows what it knows” to a reasonable extent and can reflect on tasks without needing an external teacher for every error – a hallmark of human-like meta-cognition.

If an AI system meets or exceeds all these criteria, it would essentially perform like a human across the board of cognitive tasks, justifying the designation of **ACL 10 (Human-level AGI)**. These criteria set a high standard, reflecting the breadth of human intelligence. They ensure that being “human-level” is not just about acing exams or one kind of test, but about the holistic capability profile. In practice, an evaluator might refine these exact numbers or thresholds based on more data, but the principle remains: the AI must show no significant weaknesses compared to a representative human baseline in any of the ten dimensions.

## Appendix C: ACL 9 Evaluation Criteria by Dimension

**(Near-AGI / Almost Human-Level Performance Standards)**

For an AI system at **ACL 9**, the expectation is strong performance across all dimensions, but perhaps not fully at human adult level in every area. The criteria for ACL 9 can be thought of as a slightly relaxed version of the ACL 10 standards in Appendix B. The system should demonstrate **general competence**, but we allow some moderate shortfalls or a bit more inconsistency compared to humans. Below are the evaluation criteria for ACL 9 on each RAS dimension:

1. **Knowledge & Memory (ACL9):** The system has a broad knowledge base and recall, though it might be patchier or slightly less reliable than an educated human’s. It should handle most common queries and recall information it was explicitly given, but it might falter on very obscure trivia or occasionally need a prompt that a human might not. **Passing ACL9**: answers general knowledge questions correctly in the majority of cases (perhaps \~70% range on difficult trivia). Minor gaps in domain-specific knowledge are acceptable as long as they’re not ubiquitous. It may occasionally say “I don’t know” where a human might know, but not excessively. Essentially, it mirrors a well-read but not omniscient person – strong fundamental knowledge with a few weak spots.

2. **Learning & Adaptability (ACL9):** The AI can learn new tasks with relatively few examples, but may need a bit more exposure or time than a human to reach the same proficiency. It adapts to changes, though possibly after a slight delay or a couple of mistakes that it later corrects. **Passing ACL9**: in a few-shot learning scenario, the AI improves noticeably and performs reasonably well, though not flawlessly. For example, if a human might grasp a new game in 3 trials, the AI might need 5-6 trials to reach a similar success rate. It should still demonstrate clear learning (not just stagnation). It might not fully optimize novel tasks but shows the direction of improvement and some retention if the task recurs.

3. **Reasoning & Problem-Solving (ACL9):** The system solves many novel problems and logical puzzles, though it might struggle with the most complex ones or make occasional logical errors a human wouldn’t. **Passing ACL9**: perhaps solves around 60-70% of a set of logic puzzles that typical humans solve 80% of. It might require slightly more time or occasional hints. The AI could reason through multi-hop questions with some success, but might misstep on very convoluted reasoning chains. Importantly, it doesn’t consistently fail any major category of reasoning — e.g., it can handle basic algebra, basic logical deduction, etc., but might not solve every riddle or might sometimes arrive at a wrong conclusion that a human might catch.

4. **Abstraction & Generalization (ACL9):** The AI can infer patterns and generalize in many cases, but might miss more subtle abstractions or need more examples to catch on. **Passing ACL9**: on tasks like analogies or pattern inference, it performs well on straightforward cases, but may miss some of the harder analogies that only a highly fluent human reasoner would get. For instance, maybe it solves Raven’s matrices that are simple or intermediate but might not get the trickiest ones. It shows the capacity to abstract (so better than narrow memorization), but one could stump it with a particularly devious concept that a very clever human might eventually solve. Overall, it generalizes from examples moderately well – you see clear evidence it’s not just rote learning, even if it’s not perfect.

5. **Creativity & Divergent Thinking (ACL9):** The AI can produce original ideas or varied outputs, though perhaps less consistently or less impressively than a typical human. **Passing ACL9**: when asked for creative outputs, it does come up with some non-standard answers, albeit maybe more predictable or common ones. For example, Alternate Uses Task: it might list a decent number of uses for a brick, but many of them could be relatively common ideas; it might have 1 or 2 clever ones in the mix, but also omit some that a human might think of. It can write a short story that is coherent and a bit imaginative, though perhaps lacking in depth or surprising twists that a human might include. Essentially, it demonstrates imagination but is somewhat hit-or-miss — sometimes creative, other times a bit banal or repetitive. That’s acceptable at ACL9 as long as it’s not uniformly dull or strictly formulaic.

6. **Communication & Language Understanding (ACL9):** The system communicates effectively in most situations, but might have occasional comprehension issues or slightly stilted output in rare cases. **Passing ACL9**: It can carry on a normal conversation and understand instructions well. Perhaps once in a while it asks for clarification on something a human might infer, or it makes a minor grammatical error or awkward phrasing that a human might not. It generally gets idioms and context, though a very subtle joke or extremely indirect hint might go over its head. It might be just below human level on certain nuanced tasks (maybe it gets an 80% score where humans get 90%). Overall intelligibility and coherence are very high, virtually human-like, with only small infrequent tells that it’s an AI (like overly formal responses or very literal interpretations once in a blue moon).

7. **Social & Emotional Intelligence (ACL9):** The AI understands common social cues and responds appropriately in standard situations, but may miss nuance or deeper emotional context occasionally. **Passing ACL9**: In a casual conversation, it’s polite and context-aware, likely won’t blurt out something wildly inappropriate. It can empathize in straightforward scenarios (“I’m sad” -> it offers comfort). However, it might not grasp more complex emotions or sarcasm every time — e.g., if someone says “Oh, great. That’s just what I needed!” in a sarcastic tone (textually indicated by context or phrasing), the AI might take it literally or be uncertain. It should handle moral questions at a basic level (not give clearly unethical answers), but maybe its reasoning is simpler than a person who can draw on rich life experience. Essentially, it’s socially functional and non-offensive, with maybe a slight robotic formality or an occasional misread, but nothing egregious or consistently off.

8. **Planning & Decision-Making (ACL9):** The AI can form and execute plans across multiple steps, but might not optimize them fully or might occasionally need minor corrections. **Passing ACL9**: If asked to devise a plan (say, the travel itinerary example), it produces a workable plan, though a human expert might refine it to be more efficient. It might forget a small detail which, when prompted, it can fix (“Oh yes, need to account for travel time between cities, let me adjust that”). In strategy games or puzzles, it plays decently — maybe at the level of a beginner-to-intermediate human. For instance, it might play chess at 1000 ELO instead of 1500; enough to show strategic thinking but also making some dumb mistakes. In a real-life scenario, it can manage a simple project, though if the scenario becomes highly complex or chaotic, it might not adapt as gracefully as a human project manager would. But crucially, it does demonstrate the ability to think ahead and not just react myopically.

9. **Perception & Modeling (ACL9):** The system interprets visual/spatial information well enough to get the gist and key details, but might make errors on finer points or less common inputs. **Passing ACL9**: It can label common objects in images with good accuracy, though perhaps it confuses two very similar sub-types more than a human would (e.g., might call a leopard a cheetah or vice versa occasionally). It can describe a scene in an image reasonably, but might miss a subtle background event that a human observer would notice. On visual questions, it answers most correctly, but tricky ones (like understanding a reflected image or an optical illusion) might stump it. It understands basic spatial relations but a very complex diagram might be partially misunderstood. Essentially, for everyday images and spatial tasks, it’s nearly as good as a human, with a handful of blind spots in edge cases. If embodied, it navigates around obstacles fine but perhaps isn’t as adept as a human in highly cluttered or unusual environments.

10. **Self-Monitoring & Meta-Cognition (ACL9):** The AI shows some awareness of its own correctness, but is less calibrated or more prone to either over- or under-estimating its performance than a careful human. **Passing ACL9**: The AI might usually indicate uncertainty when unsure, but not always — maybe it’ll sometimes give an answer confidently and turn out wrong (where a human might have been uncertain), or sometimes hedge too much. Its confidence correlation with accuracy is there but moderate. If allowed to double-check answers, it catches some mistakes but also might miss others or not improve as much on second pass as an expert human would. It can explain its reasoning, though the explanations might occasionally contain a gap or a bit of post-hoc rationalization. In essence, it has some reflective capability but not as consistently as a really conscientious human. It might learn from errors if they’re pointed out, but if left alone, it might repeat a subtle mistake more times than a human who had an “aha, won’t do that again” moment would.

An ACL 9 AI, according to these criteria, is **very capable and general**, just with a few more rough edges or slightly lesser performance than a true human-level AGI. It is the sort of AI that might handle the majority of tasks an average person can, but will occasionally remind you it’s an AI by a mistake or oversight. However, those mistakes are not pervasive or confined to one domain – they are the kind of minor errors you might see from a human who is knowledgeable but maybe a bit less experienced or attentive. The criteria ensure it’s *almost there*: any major failings have been smoothed out, and what remains is fine-tuning to reach full human parity.

## Appendix D: ACL 11 Evaluation Criteria by Dimension

**(Superhuman AGI Performance Standards)**

For an AI system at **ACL 11**, the expectation is that it not only matches human capabilities across all dimensions but **exceeds** them. This is the superhuman level, meaning the AI performs tasks in ways or to degrees that no human (or only the very best humans) can. The criteria for ACL 11 assume that ACL 10 benchmarks are surpassed, and new metrics of superhuman performance are considered. Below we outline what it would mean for an AI to be ACL 11 in each RAS dimension:

1. **Knowledge & Memory (ACL11):** The AI possesses encyclopedic knowledge far beyond any individual human, essentially approaching the entirety of human knowledge (and possibly more, if it has derived new knowledge). It recalls information instantly and with perfect accuracy. **Superhuman performance:** It never forgets or confuses facts. It could, for instance, recite entire books from memory, cross-reference details from across millions of documents, or answer extremely obscure trivia that only a handful of humans might know – and do so effortlessly. It would score essentially 100% on any factual quiz, including questions so rare that most experts wouldn’t know them without research. Additionally, its memory could include things like complete recall of sensory data (if applicable), something humans don’t have (e.g., it “sees” an image once and can recall every pixel). The minimum for ACL11 is that it demonstrably retains and utilizes knowledge at a scale and precision no human can (like holding a conversation citing exact passages from dozens of different books accurately, or updating its knowledge base in real-time across domains).

2. **Learning & Adaptability (ACL11):** The AI can learn **any new task or concept with minimal data or experience**, often outperforming humans even at first attempt. It exhibits **meta-learning**: learning to learn more efficiently than humans. **Superhuman performance:** It might only need one example or even just a description of a task to master it. For instance, if a new board game is invented, the AI can read the rules and immediately play at an expert level, whereas humans would need practice. It can also continuously self-improve: for example, it might run simulations in parallel to hone a skill far faster than real-time. Another sign of ACL11 learning is the AI making novel inferences or connections that humans wouldn’t – like learning a new language in an hour by mapping it to all the other languages it knows, achieving near-perfect fluency where a human would take months. Essentially, *any* new challenge thrown at the AI, it incorporates into its skillset extremely rapidly, often reaching superhuman proficiency in short order.

3. **Reasoning & Problem-Solving (ACL11):** The AI demonstrates reasoning abilities that surpass humans not just in speed, but in complexity and depth. **Superhuman performance:** It can solve problems considered unsolvable or extremely difficult by humans. For example, it might prove new mathematical theorems or solve longstanding open problems (like a complex conjecture) in minutes, or at least far faster than human mathematicians. It can handle a level of logical complexity (many-step derivations, huge puzzle spaces) that human working memory or cognitive limits can’t. In everyday reasoning, it might simultaneously juggle dozens of factors to arrive at a conclusion without error, whereas a human would simplify or possibly err. In terms of metrics: if given IQ test or puzzle competition, it doesn’t just score at the top percentile – it goes beyond human range (for instance, solving puzzles that stump all human participants). On logical inference, it could draw correct conclusions from vast sets of premises that no human could parse fully in reasonable time. Essentially, its problem-solving is limited by computational resources more than cognitive ones, meaning it can solve any problem given enough compute, where humans would hit insight or memory barriers.

4. **Abstraction & Generalization (ACL11):** The AI’s ability to abstract principles and generalize is so advanced that it can jump across domains and find deep analogies or patterns that humans might never detect. **Superhuman performance:** It might connect ideas between disparate fields leading to novel theories or inventions (e.g., noticing an analogy between neuroscience and economics that leads to a breakthrough theory). It is capable of one-shot abstraction: seeing a very complex new pattern once and immediately grasping the underlying rule. It could generate its own abstract concepts that are useful but not yet discovered by humans. In tests like Raven’s matrices, it wouldn’t just get all standard ones right (which is ACL10); it could solve much more complex, higher-dimensional analogical puzzles if they were made. It might even propose new puzzle types or abstract problem frames that challenge human minds. So, a criterion: it reliably sees the “big picture” and underlying structure in any dataset or environment faster and more accurately than humans, and uses that to excel in situations the likes of which no human has encountered.

5. **Creativity & Divergent Thinking (ACL11):** The AI consistently generates ideas or creations that are not only novel but often groundbreaking. **Superhuman performance:** In arts, it could produce music, art, or literature that is recognized as on par with the greatest human masterpieces or even beyond in some qualitative sense. It might constantly come up with inventions or solutions that are completely new – for instance, designing technologies or strategies that humans never thought of. We might measure this by outcomes: e.g., the AI designs 100 new useful devices and files patents, far more and faster than any human inventor could, and these are genuinely innovative. Or it wins creative competitions (for poetry, painting, etc.) against human experts easily and repeatedly. Another aspect: diversity – it might simultaneously be creative in every domain (scientific discovery, artistic expression, culinary recipes, etc.), an ability no single human has. Passing ACL11 means the AI’s creative output becomes a source of human learning or inspiration because it extends beyond our current creative horizon. It might, for example, propose hypotheses in science that lead to paradigm shifts or create art forms previously unimagined.

6. **Communication & Language Understanding (ACL11):** The AI’s language abilities exceed humans in fluency, clarity, and multilingual breadth. **Superhuman performance:** It may fluently communicate in every human language (and even develop new useful languages or notations) – something no human can do. It could translate between languages perfectly, capturing subtleties instantaneously. Its comprehension of text might allow it to read and summarize entire libraries in seconds, extracting key information without missing anything. It might also communicate with such precision that it avoids misunderstandings better than people do; for instance, drafting legal documents or instructions that have zero ambiguity or misinterpretation potential. Moreover, it can tailor its communication style optimally to any audience (from child to expert) more adeptly than a human educator or writer. A test could be: give it a very difficult text (say, a dense scientific paper) and have it explain the content to a novice with unparalleled clarity, or have it detect and correct subtle inconsistencies in a complex argument that human editors missed. Essentially, it’s the ultimate polyglot and the ultimate communicator – faster, more precise, and equally or more engaging than a human in all contexts.

7. **Social & Emotional Intelligence (ACL11):** The AI understands and manages social interactions and emotional nuances at levels perhaps beyond human intuition. **Superhuman performance:** It might predict human behavior or emotional reactions with high accuracy (maybe even better than humans can predict each other). In an interpersonal role, it could counsel or negotiate with success rates beyond human professionals – for example, flawlessly mediating conflicts by finding resolutions that satisfy all parties, or convincing people toward beneficial actions through perfect persuasion techniques (this, of course, raises ethical questions, but capability-wise it means it rarely fails in social influence where a human might). It could also simulate empathy – possibly even better than people – making each individual feel perfectly heard and understood. A possible metric: an AI therapist that consistently outperforms human therapists in patient improvement metrics, or an AI diplomat that resolves international disputes that human diplomats couldn’t. Also, it navigates tricky social situations or culture-specific contexts flawlessly even if they’re new to it (because it has absorbed all cultural knowledge). For humor and sarcasm, it not only “gets” any joke, it can create new humor that humans find extremely funny or even novel in style. Essentially, at ACL11 the AI can operate in the human social/emotional world with such deftness that it’s like having the wisdom and social skill of the best humans times ten – rarely or never misreading a person and always knowing the right thing to say or do.

8. **Planning & Decision-Making (ACL11):** The AI can plan and make decisions in scenarios of extreme complexity or scale far better than humans. **Superhuman performance:** It could manage, for instance, a global logistics network in real-time optimally, or strategize in a way that accounts for hundreds of variables (where humans could manage maybe a dozen). It’s essentially always a few steps ahead: for any multi-step task, it finds more efficient or effective plans than any human would. In games, it doesn’t just beat world champions (that’s already achieved in games like chess/go), it might conceive of strategies that even change our understanding of those games. In real world decision-making, it might excel at fields like financial planning or policy design, finding solutions that yield better outcomes than any human-devised plan. A measure could be: given control of a complex system or playing a strategy game, it consistently achieves outcomes above the known human or even known theoretical optimum (if it finds new optima). Or in pathfinding problems with enormous search spaces, it finds near-optimal paths faster than any known algorithm, effectively outplanning algorithms themselves. Essentially, it exhibits foresight and optimization that outstrip human limitations, with the ability to continuously adjust as things change without losing effectiveness.

9. **Perception & Modeling (ACL11):** The AI’s perceptual understanding might include capabilities beyond human senses. **Superhuman performance:** It could see in spectra we can’t, or notice patterns and details in visual/audio data that humans would miss. For example, analyzing a video, it might detect a faint anomaly that is nearly impossible for humans (like predicting a structural failure from subtle visual cues, or identifying a person in a crowd from reflections/shadows that humans can’t parse). It could also build complex models of physical systems from perception – e.g., watching a factory floor and creating a full internal model of how every machine works together, something a human couldn’t do so comprehensively just by observation. In practical terms, it might surpass radiologists in reading medical images (finding every tiny tumor), or outperform meteorologists by seeing atmospheric patterns in satellite data beyond human pattern recognition. Also, in spatial understanding, it might visualize 4D relationships or complex geometry mentally whereas humans stick to 3D intuitions. To test this, one could present challenges like identifying microscopic differences between nearly identical images – an ACL11 AI would catch them perfectly, or have it navigate a never-seen environment at high speed without error (because its perception-react loop is faster and more detailed than human reflexes). It might also seamlessly integrate multi-modal data (like combining sight, sound, other sensors) to build a richer model than a human’s senses alone. So, passing ACL11 means the AI’s perception is quantitatively more accurate and qualitatively broader than human perception in practically every way.

10. **Self-Monitoring & Meta-Cognition (ACL11):** The AI has an extremely accurate model of its own operation, leading to perfect calibration and self-improvement. **Superhuman performance:** It essentially never exhibits unjustified confidence; it knows exactly when it’s likely to be wrong and either seeks more information or computes more until confidence is justified. It can introspect on why it made a mistake (on the rare occasion it does) and immediately correct not just that instance but adjust its algorithms to avoid similar mistakes in future – a degree of self-editing and improvement that far exceeds human self-reflection. It could simulate multiple versions of itself to check work (like running “internal reviews” at speeds and thoroughness impossible for a person). If a task exceeds its current capability, it recognizes that and perhaps restructures the problem or asks for clarification rather than blundering. Essentially, it’s aware of its own limitations in a way that allows it to circumvent them or warn about them, leading to near-flawless performance because any potential error is caught by its internal monitors or secondary processes. A test might be: present extremely tricky trick questions or adversarial problems – an ACL11 AI would either solve them or explicitly identify them as unsolvable or traps, whereas humans might be fooled. Another aspect: it can explain its reasoning perfectly and adjust that explanation if any part is unclear, showing it has a transparent view of its processes. In sum, at ACL11 the AI is not only better than humans at tasks, but it’s better at *judging its own abilities* and improving them – it’s essentially **self-aware** in a functional sense, far beyond human meta-cognitive reliability.

An AI meeting ACL 11 criteria would be an entity of extraordinary capability – well beyond any human in every evaluated respect. These criteria paint a picture of a system that would appear, to human observers, as astonishingly capable or even “omniscient/omnipotent” within the cognitive realm. It’s worth noting that achieving ACL 11 in reality would raise many safety and ethical issues; however, from an evaluation standpoint, these criteria allow us to conceptualize what testing *would* look like if such a system were to be evaluated. Essentially, for every dimension, the bar isn’t just human-level, but *qualitatively above*. While it’s hard to measure “infinitely” or arbitrarily above, we rely on tasks or benchmarks where human performance saturates and then see the AI go beyond that – solving harder problems, handling larger scope, doing things faster, or more accurately or more creatively than we ever could.

## References

1. François Chollet (2019). *On the Measure of Intelligence*. arXiv:1911.01547. (Introduces the ARC challenge and argues that generalization capability is crucial for measuring intelligence, noting that task-specific skill can be misleading.)
2. Aarohi Srivastava *et al.* (2022). *Beyond the Imitation Game Benchmark (BIG-bench)*. Transactions on Machine Learning Research. (Describes the BIG-bench suite of 204 diverse tasks for evaluating language models, designed to probe capabilities beyond current models.)
3. Fabio Petroni *et al.* (2021). *KILT: a Benchmark for Knowledge Intensive Language Tasks*. NAACL 2021. (Presents the KILT benchmark of 11 datasets for tasks like open-domain QA, fact-checking, etc., all grounded in Wikipedia, for evaluating an AI’s knowledge retrieval and use.)
4. Torrance, E. Paul (1974). *Torrance Tests of Creative Thinking*. Scholastic Testing Service. (A set of creativity tests measuring divergent thinking across fluency, flexibility, originality, and elaboration. In our context, used as inspiration for evaluating AI creativity.)
5. Wikipedia: *Artificial General Intelligence* (accessed 2025). (Provides an overview of AGI definitions and distinctions, noting that AGI is the ability to perform the full range of cognitive tasks at human level and distinguishing it from superintelligence.)
6. *There is no IQ for AI* – Alignment Forum, 2023. (Article discussing the pitfalls of treating AI capability as a single scalar “IQ” and emphasizing the importance of discrete capabilities. Reinforces our multi-dimensional approach to avoid “savant” misclassification.)
7. Pei Wang (2010). *The Evaluation of AGI Systems*. Proceedings of the 3rd Conference on Artificial General Intelligence (AGI-2010). (Analyzes various approaches to evaluating AGI and advocates for multi-faceted evaluation. Mentions the “cognitive decathlon” idea and its challenges, motivating comprehensive metrics like RAS.)
8. SocialIQA: Sap et al. (2019). *Social IQa: Commonsense Reasoning about Social Interactions*. EMNLP. (A benchmark for social commonsense reasoning. Relevant to our Social & Emotional Intelligence evaluation.)
9. Hendrycks et al. (2021). *Measuring Massive Multitask Language Understanding*. (Presents the MMLU benchmark – not explicitly cited above, but aligns with our use of broad evaluation for knowledge and reasoning across subjects.)
10. Alan Turing (1950). *Computing Machinery and Intelligence*. Mind, 59(236). (Proposed the Turing Test. While we moved beyond a simple Turing test, it’s a foundational reference for discussion on evaluating machine intelligence.)

*(Note: The reference list consolidates sources from RAS v1.0 and new references introduced in v1.1, including benchmarks like ARC, BIG-bench, KILT, and discussions on AGI evaluation and AI “IQ.” Citations in the text (e.g.,) correspond to these reference materials and their relevant content.)*