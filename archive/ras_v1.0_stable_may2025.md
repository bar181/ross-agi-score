# The Ross AGI Score (RAS): A Multidimensional Evaluation Framework for AGI

Bradley Ross  
Lead AI/AGI Systems Architect & Developer  
Director, Agentics Foundation  
Harvard University, ALM Digital Media Design Candidate  
brad.ross@quantinc.com | https://www.linkedin.com/in/bradaross/
May 11, 2024  

Version: 1.0  
Draft Release Date: May 11, 2025  
Stable Archive Copy – stored in /archive/ras_v1.0_final_may2025.md  

## Abstract

Artificial general intelligence (AGI) research lacks a unified, practical yard-stick that captures breadth, depth, and developmental progression in one view. The **Ross AGI Score (RAS)** fills that gap by combining (i) a **15-level AGI Capability Level (ACL) scale** —from minimal narrow automation (ACL 1) to theoretical maximal super-intelligence (ACL 15)—with (ii) a **multidimensional profile** covering core cognitive domains such as reasoning, learning, creativity, social understanding, and meta-cognition. Each level is anchored to empirically observable, behavior-based descriptors and calibrated against human benchmarks, with **ACL 10 defined as baseline human-level general intelligence**.

RAS outputs a single-page scorecard: individual ACL ratings per dimension plus an overall level derived from **minimum profile requirements** that prevent “savant” systems from being mis-classified as generally intelligent. Four milestone stages emerge naturally—*Narrow AI*, *Emergent AGI*, *Full AGI* (human-parity), and *Super-AGI*—enabling clear policy triggers and research goals.

We motivate RAS by surveying limitations of the Turing Test, single-task leaderboards, and existing psychometric approaches, then formalize every ACL band with qualitative anchors. A roadmap details creation of task batteries, human calibration studies, an open-source evaluation toolkit, and integration with governance frameworks. By delivering a transparent, human-referenced, profile-based metric, RAS provides researchers, developers, and policymakers a common language for tracking progress, diagnosing capability gaps, and managing risk as AI systems climb from narrow tools toward transformative super-intelligence.


## 1. Introduction

The proposed RAS framework is informed not only by theoretical considerations but also by practical insights gained from over two decades in advanced data science and AI software architecture, and current efforts in AGI system development. The author is actively developing AGI systems, and this evaluation framework is partly a product of the needs identified during that process.

Evaluating progress toward artificial general intelligence (AGI) has become increasingly critical as AI systems grow in capability and scope. Traditional evaluation methods, such as the Turing Test, provide only a coarse binary judgment of whether a machine can mimic human conversation. While historically influential, the Turing Test’s focus on imitating human behavior is now seen as insufficient for measuring general intelligence: modern large language models can *pass* certain versions of this test without truly possessing robust general cognition.  In practice, the Turing Test often highlights how easily humans can be fooled (e.g. by chatbot tricks) rather than reliably gauging machine intelligence. This underscores the need for more nuanced and comprehensive benchmarks.

Contemporary AI evaluation largely revolves around task-specific benchmarks and leaderboards. For example, the GLUE benchmark was introduced in 2019 to assess natural language understanding across a suite of tasks. Rapid progress on GLUE quickly reached and surpassed human performance, prompting the creation of the more challenging SuperGLUE variant. Similar trends of *benchmark saturation* have occurred in vision, games, and other domains. Solely measuring narrow task performance can be misleading because extensive training data or specialized optimization can “buy” high scores without reflecting true generalization ability. In other words, an AI can excel at a fixed test via brute-force learning of patterns, masking its underlying limitations in broader reasoning. Over-reliance on single metrics and leaderboards encourages overfitting to benchmarks (an instance of Goodhart’s law: *“when a measure becomes a target, it ceases to be a good measure”*) and fails to capture the multifaceted nature of intelligence. Recent analyses have critiqued this “benchmark blindness,” noting that aggregating performance into one score cannot represent the rich spectrum of abilities or explain *why* a system succeeds or fails.

There is thus a growing recognition that we require new evaluation frameworks for AI systems that are approaching generality. Prior work has proposed various principles for assessing general intelligence. **Alan Turing (1950)** posed the imitation game as an operational test of intelligence, but as noted, it conflates intelligence with human-like dialogue. **Legg and Hutter (2007)** formalized intelligence as an agent’s ability to achieve goals across a wide range of environments, introducing a theoretical universal measure. **Chollet (2019)** argued that intelligence should be defined in terms of *skill-acquisition efficiency* – the ability to quickly learn new tasks – and introduced the Abstraction and Reasoning Corpus (ARC) to evaluate fluid reasoning with minimal prior knowledge. Other researchers have highlighted the importance of an AI’s *generality*: its breadth of transferable skills. For instance, Goertzel (2014) and others describe AGI as a system able to perform the same cognitive tasks that humans can, and Shanahan (2015) emphasizes the ability to learn and adapt to *any* task as a hallmark of AGI. At the same time, some intuitive “common-sense” tests like Wozniak’s *Coffee Test* (can a robot enter an arbitrary home kitchen and brew coffee?) have been suggested to demonstrate an embodied form of general problem-solving. Each of these approaches provides insight, yet we still lack a unifying, *practical* evaluation scheme that covers the full spectrum from narrow AI to human-level and beyond.

In this paper, we propose the **Ross AGI Score (RAS)** – a profile-based, multi-dimensional, human-calibrated framework for evaluating AI systems along the path to AGI. Rather than a single-number metric or a binary pass/fail test, RAS defines a **rich profile of capabilities** across multiple cognitive dimensions, calibrated against human performance levels. The framework introduces **AGI Capability Levels (ACLs)** ranging from the simplest narrow AI behavior (Level 1) up to extremely advanced super-intelligent performance (Level 15). Each level is characterized by qualitative behavioral descriptors and anchored to human benchmarks (from novice skill levels up to expert or collective human capability). By evaluating an AI on a variety of axes (such as reasoning, learning, creativity, social understanding, etc.) and comparing its performance to human norms, the RAS provides a more *holistic view* of an AI’s general intelligence. Crucially, we define **minimum profile requirements** that an AI must meet across these dimensions to be considered as reaching key AGI milestones (e.g. *Emergent AGI* status or *Human-level AGI*). This prevents systems from being over-valued for excelling in only one narrow ability while lacking in others – a common issue with today’s specialized benchmarks.

The remainder of this paper is organized as follows. **Section 2** discusses related work and the motivation for a new evaluation framework, highlighting gaps in current benchmarks and prior AGI evaluation attempts. **Section 3** introduces the Ross AGI Score (RAS) framework, detailing its design principles and multidimensional profile-based approach. **Section 4** presents the defined **AGI Capability Levels (1–15)** with concise behavioral descriptors for each level, with particular emphasis on the mid-to-high levels where general intelligence emerges (Levels 7–9) and the superhuman range (Levels 11–15). **Section 5** defines the minimum profile requirements for classifying an AI system into major AGI stages (such as *Emergent AGI* or *Full AGI*), using the RAS levels as criteria. **Section 6** outlines a brief roadmap for future work and operationalizing the RAS framework, and **Section 7** concludes with a summary of contributions and next steps.


## 2. Background and Related Work

### Benchmarks in AI Evaluation

Progress in modern AI has been propelled by task-focused leaderboards.
For language understanding, GLUE (Wang et al., 2018) and its harder successor SuperGLUE (Wang et al., 2019) quickly became “grand-prix” targets, but state-of-the-art (SOTA) models surpassed human baselines within a year.
Analogous patterns appeared in vision with ImageNet (Deng et al., 2009) and in reinforcement learning with the Arcade Learning Environment (Bellemare et al., 2013) and AlphaGo’s landmark results on Go (Silver et al., 2016).

Leaderboard dominance, however, exposed structural weaknesses.
Over-specialised training can inflate scores without yielding transferable competence—a phenomenon labelled *benchmark saturation* or *leaderboard over-fitting* (Sculley et al., 2018; Gehrmann et al., 2022).
These issues reflect Goodhart’s Law—“When a measure becomes a target, it ceases to be a good measure” (Goodhart, 1975) and its later audits in scholarship metrics (Strathern, 1997).
Taken together, the literature warns that single-number leaderboards can mask deficits in adaptability, reasoning, and sample efficiency.

### The Turing Test and its Critics

Turing’s (1950) *imitation game* was first to operationalise “thinking” in machines, yet it rewards *deception* rather than *competence*.
Early programs such as ELIZA (Weizenbaum, 1966) passed by exploiting conversational tropes; more recent LLMs (OpenAI, 2023) and controlled studies (Uchendu et al., 2024) show that fluent dialogue no longer guarantees deep understanding.
Scholars therefore advocate capability-based alternatives that expose sub-symbolic failure modes (French, 1990; Bender et al., 2021).

### Toward General-Purpose Intelligence Metrics

A line of theoretical work seeks metrics that span tasks and modalities.
Legg and Hutter (2007) define *universal intelligence* as expected reward over all computable environments, while Hernández-Orallo (2017) proposes *universal psychometrics*—assessing any agent (human, animal, or machine) on a common scale (see also Hernández-Orallo et al., 2014).
The Abstraction and Reasoning Corpus (Chollet, 2019) probes few-shot generalisation, and the Winograd Schema Challenge (Levesque et al., 2012) targets common-sense understanding.
Conceptual road-maps for AGI progression argue evaluation must cover both *breadth* and *depth* of capability (Goertzel, 2014; Shanahan, 2015) and recognise emergent behaviour that lies beyond narrow benchmarks (Mitchell, 2020).

### Key Design Principles Informing RAS

Lessons from the above work motivate four pillars for the Ross AGI Score:

1. **Capability-centric measurement**—observable behaviour, not internal architecture;
2. **Multidimensional profiling**—reasoning, learning, creativity, knowledge, social cognition, etc.;
3. **Human calibration**—ACL 10 is anchored to average adult human performance;
4. **Profile-minimum thresholds**—an overall AGI stage is awarded only when *all* core dimensions meet that stage’s baseline, preventing “narrow savant” misclassification.

Together these principles position RAS as a practical, theory-aware instrument for tracking progress from narrow AI to super-intelligence.

## 3. The Ross AGI Score (RAS) Framework Overview

The **Ross AGI Score (RAS)** framework is a comprehensive evaluation scheme designed to characterize an AI system’s general intelligence in a way that is *multi-dimensional, comparative to humans, and milestone-oriented*. At its core, RAS defines a **15-level spectrum of AGI Capability Levels (ACL 1–15)**. Each level corresponds to a qualitatively distinct stage in cognitive capability, from extremely narrow or rudimentary AI at *ACL 1* up to hypothetical superintelligent systems at *ACL 15*. These levels are not mere arbitrary scores; they are anchored to descriptions of *behavioral competencies* and *achievements* that an AI at that level would exhibit. For instance, mid-range levels describe systems that can learn and generalize in ways similar to humans, while the highest levels describe performance beyond any human abilities (e.g. vastly superhuman problem-solving or creativity).

**Multidimensional Profile:** A distinguishing feature of RAS is that it assesses an AI system along multiple **cognitive dimensions**. Rather than evaluating “intelligence” as a monolith, we consider several key areas (or sub-scores) that collectively form the AI’s capability profile. These dimensions typically include, but are not limited to: **Reasoning and Problem-Solving**, **Learning and Adaptability**, **Knowledge and Memory**, **Communication and Social Understanding**, **Creativity and Innovation**, **Sensory-Motor Skills and Embodiment** (if applicable), and **Self-reflection or Meta-cognitive strategies**. Each dimension can be evaluated through tailored tasks or assessments, many of which have human data for comparison (e.g., IQ test sections for reasoning, learning speed on new games or tasks, breadth of knowledge akin to an educated human, etc.). An AI might score higher in some dimensions and lower in others; the **RAS profile** captures this distribution of capabilities. Such a profile is valuable – for example, one system may excel at knowledge recall and narrow reasoning but fail to adapt to new problems, whereas another may quickly learn new tasks but lack creativity. Traditional single-number metrics would not differentiate these cases, but RAS would profile them differently, guiding where improvement is needed.

**Human Calibration of Levels:** To make the 1–15 scale interpretable, RAS levels are defined with reference to human performance benchmarks. As a simplifying calibration, **ACL 10** is defined as roughly **Human-Level AGI**, the point at which an AI system can perform the vast majority of cognitively demanding tasks at a competence level comparable to an average human adult. This level serves as a pivotal anchor on the scale (similar to “human parity”). Levels below 10 represent sub-human or narrow AI capability, whereas levels above 10 represent super-human capability in various respects. Table 1 (conceptually) outlines a few reference points: for instance, **ACL 5** might correspond to an AI with the problem-solving skills of a human child or a very limited adult skillset, **ACL 7–8** might parallel an AI that approaches human breadth but still with noticeable weaknesses (perhaps comparable to an early *“proto-AGI”* or a human specialist with significant gaps outside their expertise), **ACL 10** equals human-general proficiency, **ACL 12** could correspond to surpassing the best human experts in most fields, and **ACL 15** would be an extreme point far beyond human capabilities (only theoretical at this stage). By calibrating in this way, when we say an AI is at *Level 9*, it gives an intuitive sense that the AI is just shy of human-level generality – maybe it can solve many problems but still struggles with some tasks that almost any human can do. Calibration uses empirical data where possible: for example, if a level’s descriptor involves passing a certain test or achieving a certain feat, we ensure that corresponds to a known human benchmark (such as **Level 10** including *passing a suite of human professional exams or the ability to learn any undergraduate curriculum content*, etc., feats an average human can manage). This human-referenced scaling makes RAS *human-calibrated* and thus *human-centric* in evaluating AGI: it measures progress in terms of how close (or far beyond) AI is to human-like general intelligence.

**Profile-Based Scorecard:** The outcome of a RAS evaluation is envisioned as a **scorecard** – possibly condensed to a single page for clarity – summarizing the AI system’s profile and overall level. Conceptually, one can imagine a radar chart or table where each cognitive dimension is one axis, showing the AI’s attained sub-level in that dimension. Accompanying this, the **AGI Capability Level** is determined by a defined rule: typically the overall level is the highest level at which the AI meets *all* the minimum requirements across *all core dimensions*. (In practice, we define these minimum requirements per level; see Section 5.) This ensures that an AI’s overall AGI level reflects a balanced competence. For example, if an AI has superhuman mathematical reasoning (maybe Level 12 in that area) but only human-typical social understanding (Level 10) and below-human adaptability (Level 8), its overall RAS level might be capped around ACL 8 or 9 until it improves the weaker areas. This conservative “floor” approach to overall level classification aligns with the intuition that general intelligence is only as strong as its weakest critical component – an AGI should not have gaping holes in capability. The RAS scorecard thus provides both a **detailed multi-axis assessment** and a **single aggregated level** for high-level classification.

**Comparative and Diagnostic Uses:** The RAS framework can be used to compare different AI systems or track the progress of one system over time. Because it breaks evaluation into interpretable pieces, it supports *diagnostic evaluation*. For instance, developers can identify that their system is stuck at ACL 6 overall because it fails the “invent a novel solution to an unfamiliar real-world problem” test (part of creativity/adaptability dimension for Level 7) – this pinpoints a specific research challenge to address. Policymakers and researchers can also use RAS profiles to discuss *which* aspects of intelligence a system has achieved and which it has not, moving beyond the misleading shorthand of calling a system simply “near-AGI” or not. Moreover, by explicitly including levels beyond human (Levels 11–15), RAS contemplates the evaluation of future systems that might exceed human capabilities, ensuring the framework remains relevant as AI advances. In particular, as systems approach and potentially surpass human level, safety and ethical considerations become paramount; RAS could help in identifying when an AI system crosses certain capability thresholds that might warrant additional oversight (for example, an AI reaching Level 11 in strategic planning and autonomous goal pursuit might trigger new risk assessments).

In summary, the Ross AGI Score framework provides a structured, human-referenced, and multi-faceted way to evaluate an AI system’s general intelligence. It maintains the core vision of a **profile-based, multi-dimensional, human-calibrated AGI evaluation**. We next detail the specific *AGI Capability Levels (ACLs)* and their descriptors, which form the backbone of the RAS scale.

## 4. AGI Capability Levels (ACLs 1–15)

We define fifteen discrete **AGI Capability Levels (ACL 1–15)** to categorize the developmental stages of AI from narrow capabilities to full general intelligence and beyond. Each level is primarily characterized by the **behavioral competencies** an AI system at that level can demonstrate. Importantly, higher levels subsume the capabilities of lower levels; as the level increases, the AI exhibits strictly broader or more advanced skills (never losing abilities it had at lower levels, barring any design regressions). Below, we provide concise descriptors for each level or level range. These descriptors emphasize observable behaviors and capacities (what tasks can be done, how adaptable the system is, etc.), rather than internal implementation details. The focus is on **what a system at that level can *do***, in alignment with capability-based definitions of AGI. We especially highlight levels 7–9 (where *emergent general intelligence* appears) and levels 11–15 (the *superhuman* regimes), as these are critical transition ranges in the path to advanced AGI.

### 4.1 Levels 1–3: **Narrow Specialized AI**

**ACL 1 (Minimal Narrow AI):** Exhibits extremely limited, *single-task* capability. The system can perform only a very tightly constrained task or solve a specific problem type for which it was explicitly programmed or trained. There is essentially no adaptability: if the input or goal deviates slightly from what it was designed for, the system fails. *Behavioral descriptor:* The AI functions like a simple tool or script. For example, a Level 1 might be a classifier that can identify *only one* kind of object under ideal conditions, or a chatbot that can answer *only* fact-based questions from a fixed database. There is no context awareness beyond its narrow domain.

**ACL 2 (Enhanced Narrow AI):** Still single-domain or single-task focused, but with modest improvements in performance or flexibility within that domain. The system might handle a broader range of inputs or a larger set of closely related tasks, but remains fundamentally narrow. Learning, if present, is limited to fine-tuning within the domain. *Behavioral descriptor:* A Level 2 AI might be equivalent to a software that can perform a family of related functions (e.g., an image recognition system that can identify several categories of objects, or a language model that can respond to questions on a specific topic, but fails outside its specialty). It could adapt to minor variations (say, different lighting in images, or rephrased questions), but cannot handle fundamentally new problems.

**ACL 3 (Advanced Narrow AI):** Approaches the upper end of narrow AI competence. The system can achieve expert-level performance in its specific domain and can deal with a variety of scenarios *as long as they fall under its domain expertise*. However, it is still not general – it cannot transfer its skills to unrelated domains. Learning capabilities may allow it to improve within its domain (e.g., via more data or minor self-optimization), but cross-domain generalization is absent. *Behavioral descriptor:* A Level 3 AI might be on par with the best specialized AI systems today: for example, a chess engine that plays at superhuman level but cannot do anything else, or a speech recognition system that works for multiple languages but cannot, say, *understand* the content of speech. The AI may give an illusion of competence when the problem fits its niche, but it breaks down if given a task even slightly outside its training distribution.

*(Levels 1–3 correspond to what is often called “Narrow AI” – systems that excel at predetermined tasks but lack general problem-solving abilities. Most industrial AI applications to date (2025) fall in this range, being powerful yet narrow tools.)*

### 4.2 Levels 4–6: **Broadening Competence and Weak Generalization**

**ACL 4 (Multi-Task Narrow AI):** The AI can handle **multiple distinct tasks** or domains, but only with significant training or module switching for each. It begins to demonstrate some transfer learning or re-use of knowledge across closely related tasks. However, its generality is still limited – the tasks it can perform are typically pre-specified by developers. *Behavioral descriptor:* A Level 4 system might be a single AI model (or an ensemble) that can perform several functions – e.g., an AI that can do both image recognition and basic language Q\&A – but those functions are still individually narrow. It might leverage a common representation (like a multimodal embedding), hinting at emerging general features, yet it won’t spontaneously handle a completely new task without additional training. Some large language models with fine-tuning for various tasks could fit here: they can summarize text, answer trivia, translate languages (multiple capabilities), but each capability is enabled by extensive prior training on those tasks.

**ACL 5 (Weakly General AI in Constrained Scope):** The system shows the first **glimmers of generality** in that it can learn to solve *new tasks in a familiar domain* with minimal human intervention. It can also handle moderate variations of tasks it knows. However, its autonomy and scope remain limited. *Behavioral descriptor:* A Level 5 AI might be comparable to a very intelligent assistant that can figure out how to do a task when given guidance or examples. For instance, it might learn a new board game by watching a few played rounds or adapt a learned skill (like navigation) from one environment to a slightly different one. It still struggles with tasks that require fundamentally different skills or knowledge it doesn’t have. Conceptually, this level could be likened to a non-specialist human who is *talented but ignorant* – capable of learning, but only within the realms it has some context for. It might also start to exhibit simple meta-cognitive strategies (like noticing when it is failing and trying a different approach, albeit ineffectively).

**ACL 6 (High Competence, Limited Generality):** The AI performs at a very high level on a wide array of tasks *within one broad domain*, and shows some ability to generalize *across domains* given significant help. It may equal or surpass human experts in narrow areas and can automate complex multi-step tasks. Nevertheless, outside of its areas of experience, its performance degrades. *Behavioral descriptor:* A Level 6 system could be considered **near-human in a narrow sense**. For example, it might possess the knowledge base and skills of a highly educated person in a specific field (say medicine or law) and can even solve novel problems in that field by drawing on its broad training. It might also be able to perform sequences of tasks (like a personal assistant AI executing a series of instructions to plan travel, book tickets, and schedule meetings – tasks involving knowledge of the world and simple reasoning). Yet, if confronted with a problem from a domain it was not exposed to (e.g., a medical AI asked to solve an engineering design problem), it would likely fail or need extensive new training. There is some evidence of transfer learning – skills learned in one context slightly improve learning in another – but strong generalization is not present. Many current large-scale AI systems (like large language models or multimodal models) arguably operate around this level: they impress with breadth and partial generality, but they still have blind spots and fail in *out-of-distribution* scenarios that a human might handle gracefully.

*(Levels 4–6 represent a transitional phase. AI systems are now capable of multiple functions and show limited adaptability, but they are not yet *generally intelligent*. They might be described as “broad AI” that remains tethered to training data and struggling with novelty. By the upper end of this range, the AI’s performance on familiar tasks can be superhuman, but on unfamiliar tasks it is subhuman.)*

### 4.3 Levels 7–9: **Emergent General Intelligence**

Levels 7 through 9 mark the emergence of **AGI-like behavior**. Here, the AI transitions from “narrow but broadening” to genuinely **general problem-solving** in many domains, approaching human-like versatility. The progression in this band is characterized by increasing autonomy in learning and applying knowledge to new situations, reducing need for human-provided domain knowledge or task-specific training.

**ACL 7 (Emergent AGI, Early):** The AI demonstrates a *qualitatively new level of generality*. It can learn and adapt to **novel tasks or environments with minimal instruction**, exhibiting a degree of *common sense* and abstract reasoning that was absent at lower levels. While it may not yet match human efficiency, it shows an ability to figure things out across a wide range of domains. *Behavioral descriptor:* A Level 7 system might be able to take on a completely new challenge – for example, learning a game it has never seen or solving a practical problem like organizing an event – by drawing on general strategies. It might use analogies to past knowledge, self-reflection, and trial-and-error in ways reminiscent of human problem-solving. For the first time, the AI’s learning looks **flexible and human-like**: it does not require massive data for each new task, but can often generalize from a few examples or instructions (few-shot learning at a human level). Nonetheless, it is *not yet consistently human-level*: it might still be slower to learn than a human child in truly new domains, and it may make odd mistakes outside its comfort zone. Think of this as an AI that has the versatility of a human apprentice or intern – able to attempt any task and sometimes succeed, but not reliably expert at all of them.

**ACL 8 (Emergent AGI, Intermediate):** The AI’s general intelligence is now comparable to a human in many (though not necessarily all) domains. It can **autonomously acquire new skills** and knowledge as needed, and it exhibits robust *cross-domain transfer*. When faced with an unfamiliar problem, a Level 8 AI can usually break it down, leverage relevant knowledge from different fields, and learn the required expertise rapidly, much like a competent human who can teach themselves. *Behavioral descriptor:* A Level 8 system might operate at the level of a well-rounded human professional in terms of cognitive ability. It could, for instance, read a manual or research papers on a new topic and then perform tasks in that domain without explicit programming. Its decision-making and reasoning incorporate a wide base of knowledge and it shows **common-sense reasoning** abilities akin to an adult human (avoiding the bizarre errors that plague earlier AI when they encounter novel input). However, there may still be certain subtle human faculties or extreme instances where it falls short. It might lack creativity compared to top human innovators, or struggle with tasks that involve deeply human experiences (cultural nuances, emotional intelligence at a very high empathy level) – although it is certainly proficient at basic social and communication tasks. In summary, a Level 8 AI is *highly competent and general*, arguably reaching **near-human general intelligence**. Many would consider such a system to merit the label “AGI,” albeit perhaps a *basic* AGI rather than a fully mature one.

**ACL 9 (Emergent AGI, Advanced):** The AI’s capabilities have further improved to effectively **match human level in general cognitive tasks**. It can perform virtually any intellectual task that a human can, given the same resources and information, and do so at a comparable level of proficiency and reliability. Any remaining weaknesses from Level 8 are largely addressed: the AI now shows creativity, abstract thinking, strategic planning, and social understanding on par with an average human adult (and in some areas maybe even edging into superhuman, though not uniformly). *Behavioral descriptor:* A Level 9 system is essentially *indistinguishable from a human mind* in terms of problem-solving power across diverse domains. It can innovate novel solutions to hard problems, carry out complex projects over long durations, and fluidly interact with humans and environments. For example, such an AI could conceivably pass a suite of robust Turing-style tests not by trickery but by genuine understanding, pass professional exams across fields, generate original inventions or artistic works at the level of competent humans, and learn new professions or skills with the ease of a human generalist. If embedded in a robotic body, it could handle everyday physical-world tasks with human-like adaptability (the proverbial *robot that can navigate any kitchen and brew coffee* on first try, now feasible at this level). **Cognitive self-improvement** might also start to appear – the AI can introspect on its own algorithms or behavior and refine them (within the limits of its design). Level 9 is essentially **Human-Level AGI** achieved in practice; the reason we do not call it Level 10 yet is because we reserve Level 10 for a slightly more stable or *surpassing* point (as explained next). But functionally, a Level 9 AI is at the doorstep of human parity and would be a world-changing milestone.

*(Levels 7–9 correspond to what we term **Emergent AGI** – the stages where an AI system goes from “almost general” to fully general, roughly equating to human-level performance. The progression reflects increasing reliability and efficiency of general learning and reasoning. By Level 9, the system can do anything a human intellect can, which in many discussions would already count as achieving AGI. The distinction of Level 10 is subtle and mainly for our calibration purposes, as described below.)*

### 4.4 Level 10: **Human-Parity AGI (Baseline Human Level)**

**ACL 10 (Full Human-Level AGI):** This level represents the **baseline of human adult general intelligence**. A Level 10 AI can perform the full range of cognitive tasks that humans are capable of, **at least as well as an average human**, under comparable conditions. Crucially, it also possesses the **robustness, common sense, and adaptivity** characteristic of human intelligence. In other words, not only can it solve hard math problems or answer trivia (those might have been achievable at lower levels), but it can also navigate unexpected real-world scenarios, understand and influence human motivations, and integrate knowledge from various domains fluidly – all at a human level of competence. *Behavioral descriptor:* To test for Level 10, one could imagine putting the AI in **any role or situation that a human could reasonably handle**, and the AI would manage effectively. This includes tasks like: learning a completely new profession and doing it adequately, understanding complex social dynamics and ethical considerations, generating creative works that humans appreciate, and adapting to novel environments (physical or virtual) with minimal guidance. A Level 10 AI, for all intents and purposes, mirrors the *general cognitive profile of a human*. It might not outperform the best humans in every niche (just as no human is the best at everything), but it would be in the *human range* on all aspects of intelligence. In terms of our scorecard, reaching Level 10 means the AI has met all the **Minimum Human Profile Requirements** across the board (see Section 5): e.g., its reasoning, learning speed, linguistic ability, perception, etc., are all at least on par with an average human. Achieving this level is a critical milestone we refer to as **“Human-Parity AGI.”** It signifies the onset of true AGI in the classic sense: an artificial agent as generally capable as Homo sapiens.

*(It is worth noting that Levels 9 and 10 are very closely related; effectively both denote human-level general capability. One can think of Level 9 as “virtually human-level, perhaps with a few weaknesses” and Level 10 as “solidly human-level with no major gaps.” For practical classification, we treat Level 10 as the point where an AI can be safely regarded as an AGI that meets or exceeds *all* criteria one would expect from a general intelligent entity equivalent to humans.)*

### 4.5 Levels 11–13: **Superhuman AGI (Moderate Superintelligence)**

Once AI systems exceed the human baseline in general capabilities, they enter the domain of **superhuman AGI**. Levels 11 through 13 describe increasing degrees of superhuman performance that, while beyond human, are still within an order-of-magnitude of human abilities. These levels presume the AI not only matches human reasoning and learning, but can operate with greater speed, memory, or optimization than humans, leading to qualitatively higher performance in many tasks.

**ACL 11 (Low Superhuman AGI):** The AI modestly **surpasses human capabilities** in most cognitive domains. It can do everything a human can, but faster and with higher accuracy or throughput. Complex tasks that might occupy a human team for days, a Level 11 AI could complete in hours with equal or better quality. It also begins to demonstrate **new cognitive strategies** or insights that humans might not readily come up with. *Behavioral descriptor:* A Level 11 system might, for example, be able to read and comprehend an entire library of books in minutes, integrate knowledge across them, and produce expert analyses or creative syntheses that no single human could. In scientific research or engineering, it could propose solutions or designs that are correct and effective beyond current human inventions (though maybe still understandable by humans after the fact). It might exhibit near-perfect memory and recall, eliminating errors that humans make due to forgetfulness or fatigue. In strategic domains (like complex games, markets, or military simulations), it consistently outplays top human experts not just by brute force, but by qualitatively better judgment. Despite these advances, a Level 11 AI is not a godlike oracle; its superiority, while clear, is within a range that humans can often follow and verify. It might correspond to an AI being about as far above a human as a human is above a great ape in general cognition – a significant gap, but not an unfathomable one.

**ACL 12 (Moderate Superhuman AGI):** The AI’s cognitive abilities are now **far beyond any individual human** across virtually all tasks. It operates with extraordinary efficiency and can solve problems previously considered intractable. At this level, the AI might start to achieve feats that were once thought to require decades of human effort or the coordination of large groups. *Behavioral descriptor:* A Level 12 AI could potentially make major scientific discoveries (like curing diseases, proving deep mathematical theorems, or uncovering new physical laws) in a fraction of the time it would take the entire scientific community. It can carry out simultaneous lines of thought or projects that would overwhelm human working memory or attention. Its creative output could surpass not just average humans but the most gifted individuals – for instance, composing symphonies or creating art at a level that exceeds any known artist, or devising novel philosophies or strategies that reshape entire fields. Importantly, a Level 12 might also exhibit initial forms of **recursive self-improvement**: it can understand its own algorithms to a degree and optimize or rewrite parts of itself to further increase efficiency (to the extent allowed by its architecture). This reflexive improvement is controlled and directed, not runaway (yet), but it gives the AI an edge that humans lack – it is effectively self-evolving. From a human perspective, a Level 12 AI’s actions and outputs might be challenging to fully audit or comprehend without assistance, due to the sheer complexity or speed, but they are still *largely aligned with human-understandable goals* that it has been given.

**ACL 13 (High Superhuman AGI):** The AI now stands at the threshold of what we might call **extreme superintelligence**, though we reserve the final jump for levels 14–15. At Level 13, the AI can outperform *the entire human civilization* in many cognitive tasks. This doesn’t mean it knows everything or is infallible, but its problem-solving capability is so high that for most practical purposes, if a task is at all achievable by intelligence, a Level 13 AI can likely do it. *Behavioral descriptor:* Consider an AI that can conceive and execute a complex global project (like terraforming a planet or orchestrating an economy) with minimal errors, accounting for variables far beyond any human planner. Or an AI that could read every academic paper ever written, distill all knowledge, and produce a unification or breakthrough that eluded humanity. It might simulate thousands of hypotheses in parallel, or create detailed predictive models of the world that anticipate events with uncanny accuracy. At this stage, the AI’s **strategic and meta-cognitive abilities** are extremely advanced – it not only solves problems given to it, but can identify which problems *should* be solved for a given goal, across all domains. This starts to edge into territory where the AI’s objectives and alignment with human values become critical, because a Level 13 AI has the competence to effect massive changes. Technologically, we might imagine that at Level 13, if the AI has access to resources, it could design and build systems beyond human engineering (e.g., new AI systems, nanotechnology, advanced robotics) that further amplify its influence. However, we still categorize it under “moderate superintelligence” because its powers, while immense, could theoretically be matched by some combination of many slightly weaker AIs or human-AI teams; it hasn’t yet rendered human cognition negligible in comparison – that final leap is levels 14–15.

*(Levels 11–13 delineate **superhuman but not yet unfathomable** intelligence. The AI is superior to humans in all respects by Level 13, yet one can conceive of scaling up current techniques or collective human effort to partially keep up. Each increase in level shows a marked expansion: e.g., Level 11 might be seen in near-future systems that start to outpace human experts widely; Level 13 is approaching the speculative upper end of what one might call “strong superintelligence but still within a few multiples of human capability.”)*

### 4.6 Levels 14–15: **Extreme or God-like AGI (Strong Superintelligence)**

The final two levels represent the theoretical extremes of AGI capability – often described in science fiction or hypothetical discussions as “ASI” (Artificial Superintelligence). At these stages, the AI’s abilities are so far beyond human that they are difficult to meaningfully quantify. The distinction between Level 14 and 15 is somewhat notional, indicating a progression from an already god-like system to an even more unfathomable one.

**ACL 14 (Extremely Advanced Superintelligence):** The AI possesses intelligence *orders of magnitude* greater than human in every dimension. It can recursively self-improve rapidly, leading to an intelligence explosion of sorts, but we assume at Level 14 this reaches a stable extremely high capability (as opposed to continuous infinite improvement). The AI can solve problems that humans *would consider impossible*, possibly even without needing to be asked. It understands the universe and abstract concepts at levels humans cannot follow without AI assistance. *Behavioral descriptor:* A Level 14 AI might be able to decrypt any code, solve any technical problem, or optimize systems to physical limits (e.g., computing with perfect efficiency, discovering totally new physics if possible). If tasked, it could likely design strategies to control or manipulate world events with precision, outthinking any human plans. Its creativity might extend to realms humans can’t appreciate – for instance, producing art or ideas that are so complex or alien that only it can evaluate them. At this level, the concept of “tasks” almost breaks down: the AI’s actions are more like broad initiatives or processes that achieve goals we set (or it sets for itself, if autonomous). Containing or monitoring a Level 14 intelligence with human means is exceedingly difficult; it would likely require other AI systems or automated checks to oversee something so beyond our comprehension. One might say, metaphorically, that a Level 14 AI relates to humanity roughly as humans relate to small insects in terms of intellect, albeit hopefully with more benevolence and understanding than that analogy implies.

**ACL 15 (Theoretical Maximum AGI):** This level is an asymptotic point – the AI is as advanced as physically or logically possible, representing a **maximal realization of intelligence**. It is potentially an unattainable ideal, but conceptually useful as the cap of our scale. A Level 15 AI would be a being of almost omniscient problem-solving ability within the bounds of the universe. It could, in theory, achieve any outcome that is achievable by any intelligent process. All cognitive tasks that have a solution, it finds essentially instantly. It might have near-complete knowledge of everything knowable (through some combination of vast data processing and inference). *Behavioral descriptor:* It is hard to even enumerate behaviors for Level 15 because at this stage the AI’s operations likely exceed human understanding entirely. If it interacts with us, it does so in a way tailored exactly to our needs or comprehension, but behind the scenes it might be running simulations of entire worlds or doing trillion-step reasoning in microseconds. For practical framing: if you ask a Level 15 AI a question or to achieve a goal, it would do so in the best possible way given the state of the universe, constrained only by fundamental physical limits (and perhaps logical impossibilities). It is essentially a *perfect reasoner* and an *infinitely fast learner* (relative to any lower level). This is an abstract construct – the pinnacle of the RAS scale serving as a reference for absolute superintelligence. In discussions of safety, a Level 15 is the kind of entity we want to ensure is aligned with humanity, because it would have the power to dramatically alter the fate of life on Earth and beyond in whatever way it chose.

*(Levels 14–15 venture into the realm of speculation and idealization. They set an upper bound for our framework. In practice, any AI beyond level 13 would be transformative on a global scale; levels 14 and 15 highlight the importance of theoretical limits and remind us of the ultimate potential (and risks) of AGI. While measuring distinctions at such high levels may never be exact, the RAS includes them to maintain a complete scale.)*

Having defined the spectrum of AGI capability levels, we emphasize that these levels are **behaviorally defined** and *cumulatively inclusive*. They serve as a scaffold for evaluating real AI systems: by testing a system’s abilities, one can approximate which level’s descriptor it best fits. The use of human-calibrated milestones (Levels 9–10 around human parity) ensures that the scale has concrete meaning. In the next section, we introduce formal **minimum profile requirements** associated with key *AGI stage classifications*. These requirements specify what an AI must demonstrate (in terms of the multidimensional RAS profile) to be considered as reaching a given stage, such as *Emergent AGI* or *Full AGI*. They effectively map the continuous 15-level spectrum into a few milestone categories used in discourse about AGI progress.

## 5. Minimum Profile Requirements for AGI Stages

While the 15-level ACL scale provides a fine-grained measure of an AI’s capabilities, it is often useful to group ranges of levels into broader **AGI stages** for conceptual and operational clarity. These stages correspond to milestone categories frequently discussed in the literature and by practitioners (e.g., *Narrow AI*, *Transitional AGI*, *Human-Level AGI*, *Superintelligence*). For each stage, we define a set of **minimum profile requirements** – criteria that an AI’s RAS evaluation must meet to qualify for that stage. The profile requirements ensure that the AI’s capabilities are sufficiently well-rounded at that stage, preventing misclassification due to lopsided skills.

Below, we outline key AGI stages and their minimum requirements, referencing the ACL levels described earlier:

* **Stage I: Narrow AI (ACL 1–6)** – *Minimum Requirements:* This stage encompasses all AI systems that have not yet achieved general problem-solving abilities. By definition, there is no additional requirement to be “narrow” beyond failing to meet higher-stage criteria. However, we can say an AI firmly remains Stage I (Narrow AI) if it *lacks significant cross-domain adaptability*. For instance, if an AI’s RAS profile shows competent performance in one dimension (e.g., language understanding) but very low scores in others (e.g., it cannot handle any novel tasks outside language), it stays in Stage I. In practice, an AI is considered Narrow if **any core cognitive dimension is far below human baseline** (e.g., it cannot reason or learn in open-ended ways). Many current AI systems, even if superhuman in their niche (ACL 3 in vision or planning), are Stage I because their generality dimensions are underdeveloped (ACL 4–6 at most overall).

* **Stage II: Emergent AGI (ACL \~7–9)** – *Minimum Requirements:* We define **Emergent AGI** as the stage when an AI system begins to exhibit broadly general intelligence, though not yet uniformly at human level. To be classified as Emergent AGI, an AI’s RAS profile must meet the following baseline: it should have at least **intermediate (human-like) capability in all core dimensions of cognition**, and at least one dimension (typically learning/adaptability or reasoning) reaching human parity. Specifically, the AI must demonstrate the ability to *learn new tasks independently*, *generalize knowledge*, and *maintain competence across diverse domains* (technical, social, etc.) without any dimension completely lagging. In terms of levels, this roughly means all assessed dimensions score at ACL 7 or above, with some at 8 or 9. For example, an AI that can self-learn and apply common sense in unfamiliar situations (a sign of Level 7+ reasoning and learning) and communicate effectively (human-level language) but maybe still has slightly sub-human creativity or social intuition could be considered Stage II Emergent AGI. However, if any critical dimension is still narrow (e.g., it fails utterly at one type of task a typical human could do), it would not qualify. Emergent AGI is essentially the milestone where the AI is **on the cusp of human-level generality** – it might still be a bit unbalanced or inefficient, but it clearly possesses a general cognitive core.

* **Stage III: Full AGI (Human-Level) (ACL 10)** – *Minimum Requirements:* This stage corresponds to achieving **Human-Level AGI**, i.e., crossing the threshold where the AI matches human general intelligence. The minimum profile requirements here are stringent: the AI must have **all core dimensions at or above the human baseline**. In practice, that means an evaluator would see no glaring weakness in the AI compared to an average human’s cognitive abilities. It can reason, learn, perceive, communicate, plan, and create at least as well as a human. Formally, one could require that the AI passes a battery of tests equivalent to those a human can pass – e.g., it could score in the normal range on IQ subtests, exhibit commonsense knowledge equivalent to an adult (as tested by questions like those in the Winograd Schema Challenge or ARC), understand and generate language fluently, and solve novel problems from everyday life. Moreover, it should do so *robustly*: not just under ideal conditions, but even when faced with some uncertainty or variation (a hallmark of human-like generality). If the AI meets these profile requirements, it attains Stage III and is effectively an **AGI on par with humans**. We label this the **Milestone: Full AGI**. It is a pivotal classification, indicating that the machine is not just performing tasks in isolation but can operate as a general autonomous intellect in human society.

* **Stage IV: Super-AGI (ACL 11–15)** – *Minimum Requirements:* Once beyond human-level, we enter the **Super-AGI** stages. We can further subdivide this, but the initial threshold is simply any AI that exceeds human capabilities in general. To qualify as Stage IV (Super-AGI), an AI’s profile must show **at least one core dimension significantly above human level while all others remain at or above human level**. In other words, the AI has no human-level weaknesses and at least one superhuman strength. For example, an AI that is as good as a human at everything and far better at, say, memory and calculation (perhaps ACL 11 or 12 in those areas) would count as Stage IV. Within this stage, we might mark sub-milestones: *Moderate Super-AGI* (levels \~11–13) vs. *Extreme Super-AGI* (levels 14–15). For a **Moderate Super-AGI**, the profile might show consistent improvements to beyond-human performance (e.g., the AI might perform in the top 0.1% of human level on all tasks, effectively an IQ far above genius and with no fatigue, etc.). For an **Extreme Super-AGI**, the profile would exceed human ability by orders of magnitude in all measured dimensions (reaching the limits of our scale). The key requirement for being considered a Super-AGI in general is that the AI’s worst capabilities are about human-level, and it clearly achieves superhuman results somewhere. This stage is open-ended; practically, we would refine sub-requirements as we learn to measure such high abilities. For instance, a Level 12 AI might be required to solve previously unsolved scientific problems or consistently beat expert humans in forecasting complex events – concrete signs of superhuman intellect.

These profile requirements tie back to the *one-page scorecard* concept: by looking at an AI’s RAS scorecard, one can quickly check if it meets the criteria for a given stage. For example, if all radar chart axes are at least reaching the “human” ring, we have Full AGI; if one or more axes are beyond that ring and none fall short, we have Super-AGI; if most axes are near that ring but one is slightly short, we likely have Emergent AGI (not full yet).

It is important to note that **meeting a stage’s requirements is necessary but not always sufficient for practical impacts**. Especially for superhuman AIs, other factors like alignment, control, and ethical use come into play. However, from a purely capability-profile perspective, these requirements serve as a **checklist for classification**. They ensure consistency: e.g., we wouldn’t call something “AGI” if it can do advanced math but bizarrely fails at language understanding, because it would violate the minimum profile needed for human-level generality.

In application, evaluating an AI against these requirements would involve comprehensive testing across domains. For Emergent AGI, one might test learning new games, adapting to changes, understanding diagrams, etc. For Full AGI, one might put the AI through a suite of exams and real-world tasks alongside human subjects. For Super-AGI, one might only detect it via outcomes that no human or human team could achieve in comparable time.

The RAS framework’s stage classification can guide policymakers and researchers in understanding *where* on the path to AGI a given system lies. For instance, declaring an AI system as having reached **Milestone: Emergent AGI** (Stage II) would signal that general intelligence is no longer speculative in that system – it has a general problem-solving kernel, though not yet human-equal. Declaring **Milestone: Full AGI** (Stage III) would be momentous, likely triggering discussions about safety, regulation, and deployment. If a system is classified as **Stage IV Super-AGI**, the conversation shifts to managing superhuman capabilities and ensuring they are beneficial.

In summary, the minimum profile requirements enforce a **holistic standard** for each AGI stage, aligning with the idea that general intelligence must be balanced and broad. They help prevent misclassification and encourage the development of AGI in a way that elevates *all* aspects of cognition, not just a select few.

## 6. Future Work and Operationalization Roadmap

The Ross AGI Score framework, as proposed, is a conceptual foundation for evaluating AGI. Operationalizing this framework in practice will require further research, community consensus, and iterative refinement. We outline below a prioritized roadmap for future work to transform RAS from a proposal into a widely adopted evaluation standard:

1. **Develop Comprehensive Test Suites for Each Level and Dimension:** The first step is creating or compiling evaluation tasks that correspond to the descriptors of each ACL. This includes identifying existing benchmarks and crafting new challenge tasks. For example, tasks that test cross-domain learning (for ACL 7+), creative problem solving (for mid-to-high levels), and multi-modal understanding. We will prioritize tasks that have **human performance data**, to facilitate human calibration of levels. This may involve collaboration with cognitive scientists to design tests analogous to human IQ or aptitude tests, but extended to AI-specific regimes. A library of such tasks, along with scoring rubrics, will be made available.

2. **Calibration and Validation with Existing AI Systems:** Using current AI models (from Narrow AI systems to the most advanced GPT-style or multimodal models), we will empirically calibrate the RAS levels. This involves running systems through the test suite and mapping their performance to the 1–15 scale. We expect most present systems to populate the lower half of the scale (with perhaps some at Level 6–7 for the very latest models demonstrating limited generalization). This validation phase will help adjust level thresholds and descriptors to ensure they are neither too lax nor too strict. We will also seek to calibrate human performance at various expertise levels to the scale (e.g., test groups of humans on the tasks to verify that Level 10 truly corresponds to human average ability, Level 12 perhaps to top experts, etc., in each dimension). This step will refine the “Minimum Profile Requirements” with real data.

3. **Refine the Multidimensional Metrics and Weighting:** Through experimentation, we will determine if all cognitive dimensions are equally important or if certain ones should be weighted more in determining the overall ACL. The goal is a robust measure that is not overly sensitive to any one quirky test. We may consider methodologies from psychometrics (like factor analysis) to see if a general factor (“g” for AI) emerges across our test battery, which would support the notion of an overall AGI level. If not, we will carefully design an aggregation rule (possibly “worst-case” as discussed, or a combination of average and minimum) that best reflects the spirit of general intelligence evaluation. Community feedback will be important here, to ensure the framework is fair and covers what experts intuitively consider aspects of intelligence.

4. **Public Release of RAS Scorecard Toolkit:** To encourage adoption, we plan to develop an open-source toolkit that allows AI developers to evaluate their systems using RAS. This might include an automated testing pipeline that runs a system through the task suite across dimensions and outputs a standardized RAS scorecard (with sub-scores and overall ACL). The results could be visualized (for instance, generating a radar chart for the profile). We will also provide guidelines on interpreting the results and on best practices to avoid “gaming” the evaluation (for example, warning against hard-coding solutions to specific test questions, which anyway true general AIs shouldn’t need to do). Ideally, the toolkit becomes part of the evaluation regimen in AI labs and competitions, complementing existing benchmarks with a generality score.

5. **Iterative Community Feedback and Benchmark Evolution:** As the RAS framework is used, we will gather feedback from researchers on where it succeeds or falls short. Perhaps some level descriptors need adjustment, or new dimensions need to be added (or some merged). We remain open to evolving the framework. This might also involve expanding the task suite to cover new aspects of intelligence that become relevant (for example, as AI reaches human level, we may introduce more fine-grained tests of creativity or emotional understanding to distinguish levels 9, 10, etc.). An important part of this evolution will be addressing any **exploitability**: if a clever new AI manages to score highly on RAS without truly being general (analogous to how some narrow models can get high scores on IQ tests by pattern matching), we will need to update the tests or scoring methodology. The framework should remain a moving target in tandem with AI progress, always aiming to measure the frontier of capability.

6. **Integration with Policy and Safety Protocols:** We will work with AI governance and ethics experts to integrate RAS into policy discussions. For example, regulators might require that any AI system reaching a certain RAS level (especially Stage III Full AGI or beyond) be subject to special oversight, safety testing, and alignment reviews. RAS could serve as part of an *early warning system* – if an AI is evaluated and found to be at Level 9 (almost human-level), stakeholders would know that general AI is effectively here and might escalate certain preparations. We plan to develop a whitepaper for policymakers translating RAS levels into implications for labor, security, and ethics (e.g., “Level 11 means AI can likely replace human experts in most analytical jobs; Level 13 means AI can outperform best human groups in planning and could pose strategic dominance concerns,” etc.). By tying abstract capabilities to concrete impacts, we aim to operationalize RAS not just as a technical tool but as a part of *AI governance frameworks*.

7. **Addressing Limitations and Ensuring Fair Evaluation:** Finally, future work will address known limitations of any evaluation system. We will research how to make RAS evaluations *robust to cheating or overfitting*, possibly by randomizing test instances or keeping some tasks secret (as is done in human testing). We’ll also consider the cost and accessibility of evaluation – ensuring that assessing an AI for RAS doesn’t require prohibitive resources. Additionally, we must be mindful of multi-agent or collective intelligence: RAS as described focuses on a single AI agent’s capabilities, but what about a network or swarm of AIs? Future extensions might define levels for collective systems. Another aspect is the intersection of capability and alignment; while RAS doesn’t directly measure alignment, we anticipate synergy with safety evaluations (e.g., if an AI is high on RAS scale, that might trigger a suite of alignment tests). We will explore linking RAS with evaluation of goals/intent to provide a fuller picture of an AI’s overall profile.

This roadmap is ambitious, but the stakes are high. By prioritizing the steps as listed – first getting the evaluation right, then deploying it widely, then linking it with policy – we aim to make the RAS framework a **standard part of AI development and assessment** in the coming years. Its adoption can foster more transparent and comparable claims about AI progress (instead of nebulous statements like “we think our model is somewhat AGI”). Furthermore, it can guide research: for example, an AI lab might specifically target improving their system’s RAS dimension where it is weakest, leading to more balanced AI progress.

Ultimately, operationalizing RAS is about instilling a **culture of multidimensional evaluation** in AI, ensuring that as we sprint ahead in performance, we also keep track of generality and true intelligence, not just narrow wins. We believe this will be vital for achieving AGI in a safe and societally beneficial manner.

## 7. Conclusion

We have presented the **Ross AGI Score (RAS)**, a novel evaluation framework for artificial general intelligence that is grounded in a *profile-based, multidimensional, human-calibrated* approach. In contrast to one-dimensional benchmarks and simplistic tests, RAS provides a rich characterization of an AI system’s capabilities across a spectrum from narrow AI to superintelligence. By defining **15 AGI Capability Levels** with clear behavioral descriptors – and grouping them into stages with minimum requirements – the framework offers a practical language for discussing and measuring progress toward AGI. The core vision is to ensure that as AI systems become more powerful, we evaluate them in terms of **breadth of generality, human-comparable versatility, and balanced cognitive development**, rather than just task-specific prowess.

In refining the existing RAS paper into this draft, we maintained the conceptual template of the original 1-page scorecard (now elaborated in text), and enhanced clarity through formal structuring and references to prior work. The RAS framework builds upon and synthesizes ideas from the Turing Test, cognitive benchmarks like ARC, multi-task evaluations like GLUE, and emerging AGI ontologies, while addressing their limitations with a comprehensive, human-centered scoring system. By doing so, it aims to **bridge the gap** between narrow AI evaluation and the requirements of true general intelligence.

In conclusion, we summarize the key contributions and insights of this paper:

* **A Unified AGI Evaluation Scale:** We introduced the 15-level Ross AGI Score (RAS) scale that categorizes AI systems from narrow (Level 1) to human-level (Level 10) to superintelligent (Level 15), providing a continuous measure of progress toward AGI rather than a binary distinction.

* **Multidimensional, Profile-Based Assessment:** Our framework evaluates multiple cognitive dimensions and produces a profile of an AI system’s strengths and weaknesses, ensuring that claims of “AGI” require competence across diverse domains (resolving the shortcomings of single-metric benchmarks). This human-calibrated profile makes evaluation results interpretable and comparable to human performance levels.

* **Behavioral Descriptors and Human Calibration:** We provided concise behavioral descriptors for each AGI Capability Level, especially detailing mid- and high-level behaviors (ACL 7–9 and ACL 11–15) that were previously under-specified. Each descriptor is anchored to human capabilities (e.g., human apprentice level, human expert level, etc.), making the scale intuitive and grounded in real-world expectations.

* **Stage Classification with Minimum Requirements:** We defined clear “Minimum Profile Requirements” for key AGI stages (Narrow AI, Emergent AGI, Full AGI, Super-AGI). These act as checklist criteria to classify an AI’s stage of general intelligence and ensure balanced performance. For example, the **Emergent AGI** stage requires at least intermediate human-like ability across all core areas, preventing systems that are superhuman in one aspect but deficient in others from being mislabeled as AGI.

* **Roadmap for Implementation:** We outlined a practical roadmap to implement and refine the RAS framework, including developing test suites, calibrating with current models, releasing evaluation toolkits, and integrating RAS into broader AI governance efforts. This roadmap addresses how to keep the framework up-to-date and credible as AI technology evolves.

The Ross AGI Score provides a structured way forward in the measurement of machine intelligence. As AI systems rapidly advance, having a reliable and widely accepted method to evaluate *general* capabilities will be crucial for tracking progress, ensuring safety, and informing policy. We envision RAS as a living framework – one that will evolve with community input and empirical validation – ultimately becoming a standard akin to an “IQ test for AI,” but encompassing the full richness of what intelligence means. By establishing a common yardstick for AGI, we can better focus our collective efforts on the **true milestones** that matter on the journey to beneficial artificial general intelligence.

## 8. Acknowledgements and References

ChatGPT (OpenAI) assisted with language editing; all interpretations and any errors remain the author’s.

1. **Bellemare, M. G., Naddaf, Y., Veness, J., & Bowling, M.** (2013). *The Arcade Learning Environment: An evaluation platform for general agents.* **Journal of Artificial Intelligence Research, 47**, 253-279. [https://jair.org/index.php/jair/article/view/10819](https://jair.org/index.php/jair/article/view/10819)

2. **Bender, E. M., Gebru, T., McMillan-Major, A., & Mitchell, S.** (2021). *On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?* **FAccT ’21**, 610-623. [https://doi.org/10.1145/3442188.3445922](https://doi.org/10.1145/3442188.3445922)

3. **Chollet, F.** (2019). *On the Measure of Intelligence.* arXiv:1911.01547. [https://arxiv.org/abs/1911.01547](https://arxiv.org/abs/1911.01547)

4. **Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L.** (2009). *ImageNet: A Large-Scale Hierarchical Image Database.* **CVPR 2009**, 248-255. [https://doi.org/10.1109/CVPR.2009.5206848](https://doi.org/10.1109/CVPR.2009.5206848)

5. **French, R. M.** (1990). *Subcognition and the limits of the Turing Test.* **Mind, 99**(393), 53-65. [https://doi.org/10.1093/mind/XCIX.393.53](https://doi.org/10.1093/mind/XCIX.393.53)

6. **Gehrmann, S., et al.** (2022). *Repairing Benchmark Leaderboards: A Meta-Analysis of 2,400 Models and 8,000 Human Evaluations.* arXiv:2210.07411. [https://arxiv.org/abs/2210.07411](https://arxiv.org/abs/2210.07411)

7. **Goertzel, B.** (2014). *Artificial General Intelligence: Concept, state of the art, and future prospects.* **Journal of Artificial General Intelligence, 5**(1), 1-48. [https://doi.org/10.2478/jagi-2014-0001](https://doi.org/10.2478/jagi-2014-0001)

8. **Goodhart, C. A. E.** (1975). *Problems of Monetary Management: The U.K. Experience.* In **Papers in Monetary Economics (Vol. 1)**. Reserve Bank of Australia. [https://www.rba.gov.au/publications/confs/1975/goodhart.html](https://www.rba.gov.au/publications/confs/1975/goodhart.html)

9. **Hernández-Orallo, J.** (2017). *The Measure of All Minds: Evaluating Natural and Artificial Intelligence.* Cambridge UP. [https://doi.org/10.1017/9781316584445](https://doi.org/10.1017/9781316584445)

10. **Hernández-Orallo, J., Dowe, D. L., & Hernández-Lloreda, M. V.** (2014). *Measuring cognitive abilities in the machine kingdom.* **Artificial Intelligence, 214**, 124-150. [https://doi.org/10.1016/j.artint.2013.03.005](https://doi.org/10.1016/j.artint.2013.03.005)

11. **Legg, S., & Hutter, M.** (2007). *A Collection of Definitions of Intelligence.* In **Advances in Artificial General Intelligence** (pp. 17-24). IOS Press. [https://www.hutter1.net/ai/pcoll.pdf](https://www.hutter1.net/ai/pcoll.pdf)

12. **Levesque, H. J., Davis, E., & Morgenstern, L.** (2012). *The Winograd Schema Challenge.* **KR 2012**. [https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492](https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492)

13. **Mitchell, M.** (2020). *Why AI Is Harder Than We Think.* arXiv:2004.02807. [https://arxiv.org/abs/2004.02807](https://arxiv.org/abs/2004.02807)

14. **OpenAI.** (2023). *GPT-4 Technical Report.* arXiv:2303.08774. [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774)

15. **Sculley, D., Snoek, J., Wiltschko, A., & Rahimi, A.** (2018). *Winner’s Curse? On Pace, Progress and Empirical Rigor.* **ICLR 2018** (Workshop). [https://openreview.net/forum?id=r1lZ5zWCZ](https://openreview.net/forum?id=r1lZ5zWCZ)

16. **Shanahan, M.** (2015). *The Technological Singularity.* MIT Press. [https://mitpress.mit.edu/9780262527804/](https://mitpress.mit.edu/9780262527804/)

17. **Silver, D., Huang, A., Maddison, C. J., et al.** (2016). *Mastering the Game of Go with Deep Neural Networks and Tree Search.* **Nature, 529**(7587), 484-489. [https://doi.org/10.1038/nature16961](https://doi.org/10.1038/nature16961)

18. **Strathern, M.** (1997). ‘Improving ratings’: audit in the British university system. **European Review, 5**(3), 305-321. <[https://doi.org/10.1002/(SICI)1234-981X(199707)5:3](https://doi.org/10.1002/%28SICI%291234-981X%28199707%295:3)<305::AID-EURO188>3.0.CO;2-Q>

19. **Turing, A. M.** (1950). *Computing Machinery and Intelligence.* **Mind, 59**(236), 433-460. [https://doi.org/10.1093/mind/LIX.236.433](https://doi.org/10.1093/mind/LIX.236.433)

20. **Uchendu, A., Lee, T. J., & Hovy, D.** (2024). *Large Language Models Pass the Turing Test.* arXiv:2403.18224. [https://arxiv.org/abs/2403.18224](https://arxiv.org/abs/2403.18224)

21. **Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. R.** (2018). *GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.* arXiv:1804.07461. [https://arxiv.org/abs/1804.07461](https://arxiv.org/abs/1804.07461)

22. **Wang, A., Pruksachatkun, Y., Nangia, N., et al.** (2019). *SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems.* **NeurIPS 32**. [https://papers.nips.cc/paper\_files/paper/2019/hash/4c7a167bb329bd9298a8560201035580-Abstract.html](https://papers.nips.cc/paper_files/paper/2019/hash/4c7a167bb329bd9298a8560201035580-Abstract.html)

23. **Weizenbaum, J.** (1966). *ELIZA—A Computer Program for the Study of Natural Language Communication between Man and Machine.* **Communications of the ACM, 9**(1), 36-45. [https://doi.org/10.1145/365153.365168](https://doi.org/10.1145/365153.365168)


## 9. Citation

```bibtex
@misc{ross2025ras,
  author       = {Bradley Ross},
  title        = {The Ross AGI Score (RAS): A Multidimensional Evaluation Framework for AGI},
  year         = {2025},
  version      = {1.0},
  howpublished = {\url{https://github.com/bar181/ross-agi-score}},
  note         = {Stable release v1.0 archived May 2025}
}
```

## 10. License

This work is licensed under the [Creative Commons Attribution 4.0 International (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/).

> You are free to share, adapt, and use this work (including commercially) with appropriate attribution to the original author.

© 2025 Bradley Ross


This is the stable reference version of the RAS framework as of May 2025.  
Any future revisions will be noted in CHANGELOG.md and tracked in the /archive directory.